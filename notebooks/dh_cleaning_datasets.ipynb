{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, division, unicode_literals\n",
    "import os, sys\n",
    "from os.path import join\n",
    "import json\n",
    "from codecs import open\n",
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import random\n",
    "from itertools import chain\n",
    "from time import time\n",
    "from nltk.stem import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ROOT_DATA = join(os.environ[\"HOME\"], \"data/allen-ai-challenge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Datasets\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "WIKI = join(ROOT_DATA, \"parsed_wiki_data\") #top5 search wiki hits from wiki\n",
    "CK12 = join(ROOT_DATA, \"ck12_dump\") #parsing ck12\n",
    "QUIZLET = join(ROOT_DATA, 'quizlet')\n",
    "\n",
    "TRAINING = join(ROOT_DATA, \"training_set.tsv\")\n",
    "TRAINING_CLEANED = join(ROOT_DATA, \"training_set_cleaned.tsv\")\n",
    "VALIDATION = join(ROOT_DATA, \"validation_set.tsv\")\n",
    "VALIDATION_CLEANED = join(ROOT_DATA, \"validation_set_cleaned.tsv\")\n",
    "\n",
    "MERGED = join(ROOT_DATA, \"merged_corpus.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    return [stemmer.stem(w) for w in nltk.word_tokenize(text.lower()) if w not in stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 160 ms, sys: 12 ms, total: 172 ms\n",
      "Wall time: 172 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ck12_paragraphs = []\n",
    "for i, fn_short in enumerate(os.listdir(CK12)):\n",
    "    fn = join(CK12, fn_short)\n",
    "    with open(fn, encoding='utf-8', errors='ignore') as f:\n",
    "        ck12_article = json.load(f)\n",
    "        for subtitle, paragraph in ck12_article['contents'].items():\n",
    "            ck12_paragraphs.append(paragraph.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7148"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ck12_paragraphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.81 s, sys: 324 ms, total: 6.13 s\n",
      "Wall time: 6.13 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "wiki_paragraphs = []\n",
    "for i, fn_short in enumerate(os.listdir(WIKI)):\n",
    "    fn = join(WIKI, fn_short)\n",
    "    with open(fn, encoding='utf-8', errors='ignore') as f:\n",
    "        for line in (line.strip() for line in f):\n",
    "            if not line:\n",
    "                continue\n",
    "            if line.startswith('=='):\n",
    "                continue\n",
    "            wiki_paragraphs.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "561764"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wiki_paragraphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "biology.txt: 238449\n",
      "ck-12.txt: 9811\n",
      "ck 12.txt: 9811\n",
      "earth science.txt: 180614\n",
      "astronomy.txt: 190219\n",
      "chemistry.txt: 209191\n",
      "physics.txt: 196988\n",
      "ck12.txt: 4632\n",
      "CPU times: user 8.68 s, sys: 212 ms, total: 8.89 s\n",
      "Wall time: 8.89 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from StringIO import StringIO\n",
    "terms = {}\n",
    "for i, fn_short in enumerate(os.listdir(QUIZLET)):\n",
    "    term_count = 0\n",
    "    fn = join(QUIZLET, fn_short)\n",
    "    with open(fn, encoding='utf-8', errors='ignore') as f:\n",
    "        for line in f:\n",
    "            j = json.load(StringIO(line))\n",
    "            for t in j['terms']:\n",
    "                terms[t['term']] = t['definition']\n",
    "                term_count += 1\n",
    "    print(fn_short, term_count, sep=': ')\n",
    "print()\n",
    "quizlet_paragraphs = [t + ' ' + d for t, d in terms.iteritems()]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "386836"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(quizlet_paragraphs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning datasets\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords = set(stopwords.words('english') + '. , ! ? !? ?! ... ; : - â€” summary youtube www'.split())\n",
    "for t in \"no not above\".split():\n",
    "    stopwords.remove(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def text_clean(text):\n",
    "    s = re.sub(r'[^\\w\\s\\d]', '', text)\n",
    "    return [stemmer.stem(w) for w in nltk.word_tokenize(s.lower()) if w not in stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'hello', u'die', u'went', u'abov', u'45', u'5']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_clean(\"hello Died \\\\ went I Summary all of the above 45 5 www youtube\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "with open(MERGED, encoding=\"utf-8\", mode=\"w\") as f:\n",
    "    for d in chain(ck12_paragraphs, wiki_paragraphs, quizlet_paragraphs):\n",
    "        ct = text_clean(d)\n",
    "        print(*ct, sep=\" \", file=f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train/validation\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def text_clean_join(t):\n",
    "    return \" \".join(text_clean(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(TRAINING_CLEANED, encoding=\"utf-8\", mode=\"w\") as fo:\n",
    "    with open(TRAINING, encoding=\"utf-8\") as f:\n",
    "        next(f)\n",
    "        for i, l in enumerate(f):\n",
    "            [qid, q, r, aa, ab, ac, ad] = l.strip().split(\"\\t\")\n",
    "            print(qid, text_clean_join(q), r,\n",
    "                  text_clean_join(aa),\n",
    "                  text_clean_join(ab),\n",
    "                  text_clean_join(ac),\n",
    "                  text_clean_join(ad),\n",
    "                  sep=\"\\t\", file=fo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(VALIDATION_CLEANED, encoding=\"utf-8\", mode=\"w\") as fo:\n",
    "    with open(VALIDATION, encoding=\"utf-8\") as f:\n",
    "        next(f)\n",
    "        for i, l in enumerate(f):\n",
    "            [qid, q, aa, ab, ac, ad] = l.strip().split(\"\\t\")\n",
    "            print(qid, text_clean_join(q),\n",
    "                  text_clean_join(aa),\n",
    "                  text_clean_join(ab),\n",
    "                  text_clean_join(ac),\n",
    "                  text_clean_join(ad),\n",
    "                  sep=\"\\t\", file=fo)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
