{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, division, unicode_literals\n",
    "import six\n",
    "import os, sys\n",
    "from os.path import join\n",
    "import json\n",
    "from codecs import open\n",
    "from collections import defaultdict\n",
    "from operator import itemgetter\n",
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import codecs\n",
    "import random\n",
    "from time import time\n",
    "from nltk.stem import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ROOT_DATA = join(os.environ[\"HOME\"], \"data/allen-ai-challenge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Datasets\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "WIKI = join(ROOT_DATA, \"parsed_wiki_data\") #top5 search wiki hits from wiki\n",
    "CK12 = join(ROOT_DATA, \"ck12_dump\") #parsing ck12\n",
    "TRAINING = join(ROOT_DATA, \"training_set.tsv\")\n",
    "TRAINING_CLEANED = join(ROOT_DATA, \"training_set_cleaned.tsv\")\n",
    "VALIDATION = join(ROOT_DATA, \"validation_set.tsv\")\n",
    "VALIDATION_CLEANED = join(ROOT_DATA, \"validation_set_cleaned.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MERGED = join(ROOT_DATA, \"merged_corpus.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    return [stemmer.stem(w) for w in nltk.word_tokenize(text.lower()) if w not in stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 124 ms, sys: 16 ms, total: 140 ms\n",
      "Wall time: 140 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ck12_paragraphs = []\n",
    "for i, fn_short in enumerate(os.listdir(CK12)):\n",
    "    fn = join(CK12, fn_short)\n",
    "    with open(fn, encoding='utf-8', errors='ignore') as f:\n",
    "        ck12_article = json.load(f)\n",
    "        for subtitle, paragraph in ck12_article['contents'].items():\n",
    "            ck12_paragraphs.append(paragraph.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7148"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ck12_paragraphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.6 s, sys: 267 ms, total: 5.87 s\n",
      "Wall time: 5.88 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "wiki_paragraphs = []\n",
    "for i, fn_short in enumerate(os.listdir(WIKI)):\n",
    "    fn = join(WIKI, fn_short)\n",
    "    with open(fn, encoding='utf-8', errors='ignore') as f:\n",
    "        for line in (line.strip() for line in f):\n",
    "            if not line:\n",
    "                continue\n",
    "            if line.startswith('=='):\n",
    "                continue\n",
    "            wiki_paragraphs.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "561764"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wiki_paragraphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning datasets\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords = set(stopwords.words('english') + '. , ! ? !? ?! ... ; : - â€” summary youtube www'.split())\n",
    "for t in \"no not above\".split():\n",
    "    stopwords.remove(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def text_clean(text):\n",
    "    s = re.sub(r'[^\\w\\s\\d]', '', text)\n",
    "    return [stemmer.stem(w) for w in nltk.word_tokenize(s.lower()) if w not in stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'hello', u'die', u'went', u'abov', u'45', u'5']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_clean(\"hello Died \\\\ went I Summary all of the above 45 5 www youtube\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7min 1s, sys: 2.46 s, total: 7min 4s\n",
      "Wall time: 7min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with open(MERGED, encoding=\"utf-8\", mode=\"w\") as f:\n",
    "    for d in chain(ck12_paragraphs, wiki_paragraphs):\n",
    "        ct = text_clean(d)\n",
    "        print(*ct, sep=\" \", file=f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train/validation\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def text_clean_join(t):\n",
    "    return \" \".join(text_clean(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(TRAINING_CLEANED, encoding=\"utf-8\", mode=\"w\") as fo:\n",
    "    with open(TRAINING, encoding=\"utf-8\") as f:\n",
    "        next(f)\n",
    "        for i, l in enumerate(f):\n",
    "            [qid, q, r, aa, ab, ac, ad] = l.strip().split(\"\\t\")\n",
    "            print(qid, text_clean_join(q), r,\n",
    "                  text_clean_join(aa),\n",
    "                  text_clean_join(ab),\n",
    "                  text_clean_join(ac),\n",
    "                  text_clean_join(ad),\n",
    "                  sep=\"\\t\", file=fo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(VALIDATION_CLEANED, encoding=\"utf-8\", mode=\"w\") as fo:\n",
    "    with open(VALIDATION, encoding=\"utf-8\") as f:\n",
    "        next(f)\n",
    "        for i, l in enumerate(f):\n",
    "            [qid, q, aa, ab, ac, ad] = l.strip().split(\"\\t\")\n",
    "            print(qid, text_clean_join(q),\n",
    "                  text_clean_join(aa),\n",
    "                  text_clean_join(ab),\n",
    "                  text_clean_join(ac),\n",
    "                  text_clean_join(ad),\n",
    "                  sep=\"\\t\", file=fo)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
