{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, division, unicode_literals\n",
    "import os, sys\n",
    "from os.path import join\n",
    "import json\n",
    "from codecs import open\n",
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import random\n",
    "from itertools import chain\n",
    "from time import time\n",
    "from nltk.stem import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ROOT_DATA = join(os.environ[\"HOME\"], \"data/allen-ai-challenge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Datasets\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "WIKI = join(ROOT_DATA, \"parsed_wiki_data\") #top5 search wiki hits from wiki\n",
    "CK12 = join(ROOT_DATA, \"ck12_dump\") #parsing ck12\n",
    "QUIZLET = join(ROOT_DATA, 'quizlet')\n",
    "TOPKEK = join(ROOT_DATA, 'studystack')\n",
    "KIDSDICT = join(ROOT_DATA, 'kidsdict')\n",
    "\n",
    "TRAINING = join(ROOT_DATA, \"training_set.tsv\")\n",
    "TRAINING_CLEANED = join(ROOT_DATA, \"training_set_cleaned.tsv\")\n",
    "VALIDATION = join(ROOT_DATA, \"validation_set.tsv\")\n",
    "VALIDATION_CLEANED = join(ROOT_DATA, \"validation_set_cleaned.tsv\")\n",
    "\n",
    "CORPUS_PARAGRAPH = join(ROOT_DATA, \"corpus_paragraph_unstable.txt\")\n",
    "#CORPUS_BRICKS = join(ROOT_DATA, \"corpus_bricks_roman_2.txt\")\n",
    "#CORPUS_SENTS = join(ROOT_DATA, \"corpus_sentences_roman_2.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    return [stemmer.stem(w) for w in nltk.word_tokenize(text.lower()) if w not in stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 133 ms, sys: 17.3 ms, total: 150 ms\n",
      "Wall time: 153 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ck12_paragraphs = []\n",
    "for i, fn_short in enumerate(os.listdir(CK12)):\n",
    "    fn = join(CK12, fn_short)\n",
    "    with open(fn, encoding='utf-8', errors='ignore') as f:\n",
    "        ck12_article = json.load(f)\n",
    "        for subtitle, paragraph in ck12_article['contents'].items():\n",
    "            ck12_paragraphs.append(paragraph.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7148"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ck12_paragraphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.39 s, sys: 461 ms, total: 5.85 s\n",
      "Wall time: 5.86 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "wiki_paragraphs = []\n",
    "for i, fn_short in enumerate(os.listdir(WIKI)):\n",
    "    fn = join(WIKI, fn_short)\n",
    "    with open(fn, encoding='utf-8', errors='ignore') as f:\n",
    "        for line in (line.strip() for line in f):\n",
    "            if not line:\n",
    "                continue\n",
    "            if line.startswith('=='):\n",
    "                continue\n",
    "            wiki_paragraphs.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "561764"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wiki_paragraphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "geology.txt: 211689\n",
      "biology.txt: 238449\n",
      "chem.txt: 186910\n",
      "ck-12.txt: 9811\n",
      "ck 12.txt: 9811\n",
      "space.txt: 103268\n",
      "lifescience.txt: 7104\n",
      "earthscience.txt: 8281\n",
      "climate.txt: 107490\n",
      "anatomy.txt: 275689\n",
      "physicalscience.txt: 4753\n",
      "physiology.txt: 288464\n",
      "earth science.txt: 180614\n",
      "botany.txt: 247006\n",
      "astronomy.txt: 190219\n",
      "science.txt: 164241\n",
      "life science.txt: 166535\n",
      "physical science.txt: 191991\n",
      "chemistry.txt: 209191\n",
      "physics.txt: 196988\n",
      "genetics.txt: 181100\n",
      "ck12.txt: 4632\n",
      "DNA.txt: 125874\n",
      "\n",
      "CPU times: user 27.1 s, sys: 750 ms, total: 27.8 s\n",
      "Wall time: 27.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from StringIO import StringIO\n",
    "terms = {}\n",
    "for i, fn_short in enumerate(os.listdir(QUIZLET)):\n",
    "    term_count = 0\n",
    "    fn = join(QUIZLET, fn_short)\n",
    "    with open(fn, encoding='utf-8', errors='ignore') as f:\n",
    "        for line in f:\n",
    "            j = json.load(StringIO(line))\n",
    "            for t in j['terms']:\n",
    "                terms[t['term']] = t['definition']\n",
    "                term_count += 1\n",
    "    print(fn_short, term_count, sep=': ')\n",
    "print()\n",
    "quizlet_paragraphs = [t + ' ' + d for t, d in terms.iteritems()]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1192040"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(quizlet_paragraphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ch_docs.txt: 426147\n",
      "gen_docs.txt: 17119\n",
      "es_docs.txt: 459625\n",
      "bio_docs.txt: 1736698\n",
      "apgeo_docs.txt: 81173\n",
      "ph_docs.txt: 106345\n",
      "science_docs.txt: 1094720\n",
      "geo_docs.txt: 332818\n",
      "anth_docs.txt: 36011\n",
      "ps_docs.txt: 413622\n",
      "\n",
      "CPU times: user 6.56 s, sys: 2.1 s, total: 8.67 s\n",
      "Wall time: 8.63 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "topkek_cards = []\n",
    "for i, path in enumerate(os.listdir(TOPKEK)):\n",
    "    fn = join(TOPKEK, path)\n",
    "    with open(fn, encoding='utf-8', errors='ignore') as f:\n",
    "        docs = f.readlines()\n",
    "        for d in docs:\n",
    "            topkek_cards.append(d.strip())\n",
    "        print(path, len(docs), sep=': ')\n",
    "topkek_cards = set(topkek_cards)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1927951"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(topkek_cards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_docs.txt: 10410\n",
      "\n",
      "CPU times: user 102 ms, sys: 4.14 ms, total: 106 ms\n",
      "Wall time: 103 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "kidsdict = []\n",
    "for i, path in enumerate(os.listdir(KIDSDICT)):\n",
    "    fn = join(KIDSDICT, path)\n",
    "    with open(fn, encoding='utf-8', errors='ignore') as f:\n",
    "        docs = f.readlines()\n",
    "        for d in docs:\n",
    "            kidsdict.append(d.strip())\n",
    "        print(path, len(docs), sep=': ')\n",
    "kidsdict = set(kidsdict)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10407"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(kidsdict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning datasets\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords = set(stopwords.words('english') + '. , ! ? !? ?! ... ; : - â€” summary youtube www'.split())\n",
    "for t in \"no not above\".split():\n",
    "    stopwords.remove(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def text_clean(text):\n",
    "    s = re.sub(r'[^\\w\\s\\d]', '', text)\n",
    "    return [stemmer.stem(w) for w in nltk.word_tokenize(s.lower()) if w not in stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'hello', u'die', u'went', u'abov', u'45', u'5']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_clean(\"hello Died \\\\ went I Summary all of the above 45 5 www youtube\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-46-9f5c9db53af0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mu'time'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34mu''\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34mu'c = 0\\nwith open(CORPUS_BRICKS, encoding=\"utf-8\", mode=\"w\") as f:\\n    for d in chain(ck12_paragraphs, wiki_paragraphs, quizlet_paragraphs, topkek_cards):\\n        sents = list(nltk.sent_tokenize(d))\\n        if len(sents) == 1:\\n            ct = text_clean(sents[0])\\n            print(*ct, sep=\" \", file=f)\\n            c += 1\\n        else:\\n            for s1, s2 in zip(sents[:-1], sents[1:]):\\n                ct = text_clean(s1 + \\' \\' + s2)\\n                print(*ct, sep=\" \", file=f)\\n                c += 1\\nprint(\\'Wrote\\', c, \\'bricks\\')'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.pyc\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[1;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[0;32m   2291\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2292\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2293\u001b[1;33m                 \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2294\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2295\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<decorator-gen-60>\u001b[0m in \u001b[0;36mtime\u001b[1;34m(self, line, cell, local_ns)\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/IPython/core/magic.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(f, *a, **k)\u001b[0m\n\u001b[0;32m    191\u001b[0m     \u001b[1;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 193\u001b[1;33m         \u001b[0mcall\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    194\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/IPython/core/magics/execution.pyc\u001b[0m in \u001b[0;36mtime\u001b[1;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[0;32m   1165\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1166\u001b[0m             \u001b[0mst\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1167\u001b[1;33m             \u001b[1;32mexec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1168\u001b[0m             \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1169\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m<ipython-input-45-e79ff81883a9>\u001b[0m in \u001b[0;36mtext_clean\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mtext_clean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr'[^\\w\\s\\d]'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mstemmer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/nltk/tokenize/__init__.pyc\u001b[0m in \u001b[0;36mword_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m    103\u001b[0m     \"\"\"\n\u001b[0;32m    104\u001b[0m     return [token for sent in sent_tokenize(text, language)\n\u001b[1;32m--> 105\u001b[1;33m             for token in _treebank_word_tokenize(sent)]\n\u001b[0m\u001b[0;32m    106\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/nltk/tokenize/treebank.pyc\u001b[0m in \u001b[0;36mtokenize\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[1;31m#starting quotes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m         \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr'^\\\"'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34mr'``'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m         \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr'(``)'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34mr' \\1 '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m         \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr'([ (\\[{<])\"'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34mr'\\1 `` '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/lib/python2.7/re.pyc\u001b[0m in \u001b[0;36msub\u001b[1;34m(pattern, repl, string, count, flags)\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[0ma\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mit\u001b[0m\u001b[0;31m'\u001b[0m\u001b[0ms\u001b[0m \u001b[0mpassed\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmatch\u001b[0m \u001b[0mobject\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mmust\u001b[0m \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    150\u001b[0m     a replacement string to be used.\"\"\"\n\u001b[1;32m--> 151\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_compile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrepl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    152\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0msubn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrepl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "c = 0\n",
    "with open(CORPUS_BRICKS, encoding=\"utf-8\", mode=\"w\") as f:\n",
    "    for d in chain(ck12_paragraphs, wiki_paragraphs, quizlet_paragraphs, topkek_cards):\n",
    "        sents = list(nltk.sent_tokenize(d))\n",
    "        if len(sents) == 1:\n",
    "            ct = text_clean(sents[0])\n",
    "            print(*ct, sep=\" \", file=f)\n",
    "            c += 1\n",
    "        else:\n",
    "            for s1, s2 in zip(sents[:-1], sents[1:]):\n",
    "                ct = text_clean(s1 + ' ' + s2)\n",
    "                print(*ct, sep=\" \", file=f)\n",
    "                c += 1\n",
    "print('Wrote', c, 'bricks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "c = 0\n",
    "with open(CORPUS_SENTS, encoding=\"utf-8\", mode=\"w\") as f:\n",
    "    for d in chain(ck12_paragraphs, wiki_paragraphs, quizlet_paragraphs, topkek_cards):\n",
    "        for s in nltk.sent_tokenize(d):\n",
    "            ct = text_clean(s)\n",
    "            print(*ct, sep=\" \", file=f)\n",
    "            c += 1\n",
    "print('Wrote', c, 'sents')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "c = 0\n",
    "n = 50\n",
    "overlap = 10\n",
    "with open(CORPUS_PARAGRAPH, encoding=\"utf-8\", mode=\"w\") as f:\n",
    "    for d in chain(ck12_paragraphs, wiki_paragraphs,\n",
    "                   quizlet_paragraphs, topkek_cards, kidsdict):\n",
    "        ct = text_clean(d)\n",
    "        if len(ct) >= 100:\n",
    "            for q in [ct[i:i+n] for i in range(0, len(ct) - n, n-overlap)]:\n",
    "                print(*q, sep=\" \", file=f)\n",
    "                c += 1\n",
    "        else:\n",
    "            print(*ct, sep=\" \", file=f)\n",
    "            c += 1\n",
    "print('Wrote', c, 'paragraphs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train/validation\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def text_clean_join(t):\n",
    "    return \" \".join(text_clean(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(TRAINING_CLEANED, encoding=\"utf-8\", mode=\"w\") as fo:\n",
    "    with open(TRAINING, encoding=\"utf-8\") as f:\n",
    "        next(f)\n",
    "        for i, l in enumerate(f):\n",
    "            [qid, q, r, aa, ab, ac, ad] = l.strip().split(\"\\t\")\n",
    "            print(qid, text_clean_join(q), r,\n",
    "                  text_clean_join(aa),\n",
    "                  text_clean_join(ab),\n",
    "                  text_clean_join(ac),\n",
    "                  text_clean_join(ad),\n",
    "                  sep=\"\\t\", file=fo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(VALIDATION_CLEANED, encoding=\"utf-8\", mode=\"w\") as fo:\n",
    "    with open(VALIDATION, encoding=\"utf-8\") as f:\n",
    "        next(f)\n",
    "        for i, l in enumerate(f):\n",
    "            [qid, q, aa, ab, ac, ad] = l.strip().split(\"\\t\")\n",
    "            print(qid, text_clean_join(q),\n",
    "                  text_clean_join(aa),\n",
    "                  text_clean_join(ab),\n",
    "                  text_clean_join(ac),\n",
    "                  text_clean_join(ad),\n",
    "                  sep=\"\\t\", file=fo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'+'.join('104080  germin compos mollusk kingdom   scallop protista        dna molecul     '.split())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
