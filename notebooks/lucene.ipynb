{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division, unicode_literals\n",
    "import six\n",
    "import os\n",
    "from os.path import join\n",
    "import json\n",
    "from codecs import open\n",
    "from collections import defaultdict\n",
    "from operator import itemgetter\n",
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import codecs\n",
    "\n",
    "import spacy\n",
    "\n",
    "from nltk.stem import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import spacy.en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords = set(stopwords.words('english') + '. , ! ? ; - : —'.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nlp = spacy.en.English()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    return [stemmer.stem(w) for w in nltk.word_tokenize(text.lower()) if w not in stopwords]\n",
    "    #return [stemmer.stem(w.text) for w in nlp(text) if not w.is_stop and not w.is_punct and not w.is_space]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'hello']"
      ]
     },
     "execution_count": 34,
     "output_type": "execute_result",
     "metadata": {}
    }
   ],
   "source": [
    "tokenize(\"hello i am! —\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 35,
     "output_type": "execute_result",
     "metadata": {}
    }
   ],
   "source": [
    "\"die\" in stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'cap', u'acid']"
      ]
     },
     "execution_count": 36,
     "output_type": "execute_result",
     "metadata": {}
    }
   ],
   "source": [
    "tokenize(\"Caps - acid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lucene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from org.apache.lucene.analysis.standard import StandardAnalyzer\n",
    "from org.apache.lucene.document import Document, Field\n",
    "from org.apache.lucene.index import IndexWriter, IndexWriterConfig, IndexReader\n",
    "from org.apache.lucene.search import IndexSearcher\n",
    "from org.apache.lucene.search import Sort, SortField\n",
    "from org.apache.lucene.queryparser.classic import QueryParser\n",
    "from org.apache.lucene.store import SimpleFSDirectory\n",
    "from org.apache.lucene.util import Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from java.io import File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = join(os.environ['HOME'], 'data/allen-ai-challenge')\n",
    "WIKI_DIR = join(DATA_DIR, 'wiki_dump')\n",
    "CK12_DIR = join(DATA_DIR, 'ck12_dump')\n",
    "WIKI_CK12_DIR = join(DATA_DIR, 'wiki_subset')\n",
    "TRAINING_SET = join(DATA_DIR, 'training_set.tsv')\n",
    "VALIDATION_SET = join(DATA_DIR, 'validation_set.tsv')\n",
    "TRAINING_SET_MERGED = join(DATA_DIR, 'training_set_merged.tsv')\n",
    "# INDEX_DIR = join(DATA_DIR, 'index-wiki-ck12')\n",
    "# INDEX_DIR = join(DATA_DIR, 'index-ck12-stem')\n",
    "INDEX_DIR = join(DATA_DIR, 'index-all-wiki_ck12_search4')\n",
    "SUBMISSION = join(DATA_DIR, 'submissions/lucene_wiki_ck12_16jan.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<jcc.JCCEnv at 0x1085c2b70>"
      ]
     },
     "execution_count": 41,
     "output_type": "execute_result",
     "metadata": {}
    }
   ],
   "source": [
    "lucene.initVM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = StandardAnalyzer(Version.LUCENE_4_10_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "writerConfig = IndexWriterConfig(Version.LUCENE_4_10_1, StandardAnalyzer())\n",
    "writer = IndexWriter(SimpleFSDirectory(File(INDEX_DIR)), writerConfig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "wiki from ck12\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7min, sys: 5.87 s, total: 7min 6s\n",
      "Wall time: 7min 17s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for i, fn_short in enumerate(os.listdir(WIKI_CK12_DIR)):\n",
    "    fn = join(WIKI_CK12_DIR, fn_short)\n",
    "    with open(fn, encoding='utf-8', errors='ignore') as f:\n",
    "        wiki_article = json.load(f)\n",
    "        title, summary, content = wiki_article\n",
    "        content = title + \" \" + summary + \" \" + content\n",
    "        content = \" \".join(tokenize(content))\n",
    "        doc = Document()\n",
    "        doc.add(Field(\"text\", content, Field.Store.YES, Field.Index.ANALYZED))\n",
    "        writer.addDocument(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9423"
      ]
     },
     "execution_count": 45,
     "output_type": "execute_result",
     "metadata": {}
    }
   ],
   "source": [
    "writer.numDocs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ck12 parsed\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-9450abe2aa11>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mu'time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mu''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mu'for i, fn_short in enumerate(os.listdir(CK12_DIR)):\\n    fn = join(CK12_DIR, fn_short)\\n    with open(fn, encoding=\\'utf-8\\', errors=\\'ignore\\') as f:\\n        ck12_article = json.load(f)\\n        content = ck12_article[\"science\"] + \" \" + ck12_article[\"concept\"] + \" \" + \\'. \\'.join([paragraph for subtitle, paragraph in ck12_article[\\'contents\\'].items()])\\n        content = \" \".join(tokenize(content))\\n        doc = Document()\\n        doc.add(Field(\"text\", content, Field.Store.YES, Field.Index.ANALYZED))\\n        writer.addDocument(doc)'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/Pavel/anaconda/lib/python2.7/site-packages/IPython/core/interactiveshell.pyc\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2262\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2263\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2264\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2265\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Pavel/anaconda/lib/python2.7/site-packages/IPython/core/magics/execution.pyc\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
      "\u001b[0;32m/Users/Pavel/anaconda/lib/python2.7/site-packages/IPython/core/magic.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    191\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Pavel/anaconda/lib/python2.7/site-packages/IPython/core/magics/execution.pyc\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1164\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1165\u001b[0m             \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1166\u001b[0;31m             \u001b[0;32mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1167\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1168\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/Users/Pavel/anaconda/lib/python2.7/json/__init__.pyc\u001b[0m in \u001b[0;36mload\u001b[0;34m(fp, encoding, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    289\u001b[0m         \u001b[0mparse_float\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparse_float\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparse_int\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparse_int\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m         \u001b[0mparse_constant\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparse_constant\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobject_pairs_hook\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobject_pairs_hook\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 291\u001b[0;31m         **kw)\n\u001b[0m\u001b[1;32m    292\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Pavel/anaconda/lib/python2.7/json/__init__.pyc\u001b[0m in \u001b[0;36mloads\u001b[0;34m(s, encoding, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    337\u001b[0m             \u001b[0mparse_int\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mparse_float\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m             parse_constant is None and object_pairs_hook is None and not kw):\n\u001b[0;32m--> 339\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_decoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    340\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m         \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mJSONDecoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Pavel/anaconda/lib/python2.7/json/decoder.pyc\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m         \"\"\"\n\u001b[0;32m--> 364\u001b[0;31m         \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    365\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Pavel/anaconda/lib/python2.7/json/decoder.pyc\u001b[0m in \u001b[0;36mraw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    378\u001b[0m         \"\"\"\n\u001b[1;32m    379\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 380\u001b[0;31m             \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscan_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    381\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"No JSON object could be decoded\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for i, fn_short in enumerate(os.listdir(CK12_DIR)):\n",
    "    fn = join(CK12_DIR, fn_short)\n",
    "    with open(fn, encoding='utf-8', errors='ignore') as f:\n",
    "        ck12_article = json.load(f)\n",
    "        content = ck12_article[\"science\"] + \" \" + ck12_article[\"concept\"] + \" \" + '. '.join([paragraph for subtitle, paragraph in ck12_article['contents'].items()])\n",
    "        content = \" \".join(tokenize(content))\n",
    "        doc = Document()\n",
    "        doc.add(Field(\"text\", content, Field.Store.YES, Field.Index.ANALYZED))\n",
    "        writer.addDocument(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1630"
      ]
     },
     "execution_count": 16,
     "output_type": "execute_result",
     "metadata": {}
    }
   ],
   "source": [
    "writer.numDocs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, fn_short in enumerate(os.listdir(WIKI_DIR)):\n",
    "#     fn = join(WIKI_DIR, fn_short)\n",
    "#     with open(fn, encoding='utf-8', errors='ignore') as f:\n",
    "#         wiki_article = json.load(f)\n",
    "#         a, b, content = wiki_article\n",
    "#         print(content)\n",
    "# #         content = \" \".join(tokenize(content[0]))\n",
    "#         if i > 2:\n",
    "#             break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from wiki science category\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6min 24s, sys: 5.16 s, total: 6min 29s\n",
      "Wall time: 6min 27s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for i, fn_short in enumerate(os.listdir(WIKI_DIR)):\n",
    "    fn = join(WIKI_DIR, fn_short)\n",
    "    with open(fn, encoding='utf-8', errors='ignore') as f:\n",
    "        wiki_article = json.load(f)\n",
    "        _, summary, content = wiki_article\n",
    "        content = summary + \" \" + content\n",
    "        content = \" \".join(tokenize(content))\n",
    "        doc = Document()\n",
    "        doc.add(Field(\"text\", content, Field.Store.YES, Field.Index.ANALYZED))\n",
    "        writer.addDocument(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13635"
      ]
     },
     "execution_count": 18,
     "output_type": "execute_result",
     "metadata": {}
    }
   ],
   "source": [
    "writer.numDocs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import telepot\n",
    "def telegram_notify(msg):\n",
    "    token = \"178350795:AAFG7yae2SSt52GLek2bKS43oK7BaywWxRw\"\n",
    "    bot = telepot.Bot(token)\n",
    "    b = bot.getMe()\n",
    "    response = bot.getUpdates()\n",
    "    bot.sendMessage(31747780, msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "telegram_notify(\"lucene done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13635"
      ]
     },
     "execution_count": 21,
     "output_type": "execute_result",
     "metadata": {}
    }
   ],
   "source": [
    "writer.numDocs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iter_data(datafile, with_correct=True):\n",
    "    with open(datafile, encoding='utf-8', errors='ignore') as f:\n",
    "        next(f)\n",
    "        for l in f:\n",
    "            if with_correct:\n",
    "                idd, q, correct, aa, ab, ac, ad = l.strip().split(\"\\t\")\n",
    "            else:\n",
    "                idd, q, aa, ab, ac, ad = l.strip().split(\"\\t\")\n",
    "                correct = \"no\"\n",
    "            q, aa, ab, ac, ad = [' '.join(tokenize(x)) for x in [q, aa, ab, ac, ad]]\n",
    "            yield {\"idd\": idd, \"q\": q, \"correct\": correct, \"aa\": aa, \"ab\": ab, \"ac\": ac, \"ad\": ad}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 23.6 s, sys: 2.03 s, total: 25.6 s\n",
      "Wall time: 24.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "res = defaultdict(list)\n",
    "MAX = 100\n",
    "docs_per_q = range(1, 20)\n",
    "\n",
    "analyzer = StandardAnalyzer(Version.LUCENE_4_10_1)\n",
    "reader = IndexReader.open(SimpleFSDirectory(File(INDEX_DIR)))\n",
    "searcher = IndexSearcher(reader)\n",
    "\n",
    "for row in iter_data(TRAINING_SET):\n",
    "    queries = [row['aa'], row['ab'], row['ac'], row['ad']]\n",
    "    queries = [row['q'] + ' ' + q  for q in queries]\n",
    "    scores = defaultdict(list)\n",
    "    for q in queries:\n",
    "        query = QueryParser(Version.LUCENE_4_10_1, \"text\", analyzer).parse(re.sub(\"[^a-zA-Z0-9]\",\" \", q))\n",
    "        #query = QueryParser(Version.LUCENE_30, \"text\", analyzer).parse(re.sub(\"[/^]\", \"\\^\", q))\n",
    "        hits = searcher.search(query, MAX)\n",
    "        doc_importance = [hit.score for hit in hits.scoreDocs]\n",
    "        for n in docs_per_q:\n",
    "            scores[n].append(sum(doc_importance[:n]))\n",
    "      \n",
    "    for n in docs_per_q:\n",
    "        res[n].append(['A','B','C','D'][np.argmax(scores[n])] == row[\"correct\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0.428\n",
      "2 0.43\n",
      "3 0.4328\n",
      "4 0.4332\n",
      "5 0.434\n",
      "6 0.4384\n",
      "7 0.4364\n",
      "8 0.4336\n",
      "9 0.4288\n",
      "10 0.4304\n",
      "11 0.4252\n",
      "12 0.4248\n",
      "13 0.4252\n",
      "14 0.4248\n",
      "15 0.424\n",
      "16 0.4216\n",
      "17 0.4224\n",
      "18 0.4212\n",
      "19 0.4184\n"
     ]
    }
   ],
   "source": [
    "for x in sorted(res):\n",
    "    print(x, np.mean(res[x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 14s, sys: 8.64 s, total: 1min 23s\n",
      "Wall time: 1min 25s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "MAX = 100\n",
    "docs_per_q = 4\n",
    "\n",
    "analyzer = StandardAnalyzer(Version.LUCENE_4_10_1)\n",
    "reader = IndexReader.open(SimpleFSDirectory(File(INDEX_DIR)))\n",
    "searcher = IndexSearcher(reader)\n",
    "\n",
    "with open(SUBMISSION, \"w\") as s:\n",
    "    s.write(\"id,correctAnswer\\n\")\n",
    "    for row in iter_data(VALIDATION_SET, False):\n",
    "        queries = [row['aa'], row['ab'], row['ac'], row['ad']]\n",
    "        queries = [row['q'] + ' ' + q  for q in queries]\n",
    "        scores = []\n",
    "        for q in queries:\n",
    "            query = QueryParser(Version.LUCENE_4_10_1, \"text\", analyzer).parse(re.sub(\"[^a-zA-Z0-9]\",\" \", q))\n",
    "            #query = QueryParser(Version.LUCENE_30, \"text\", analyzer).parse(re.sub(\"[/^]\", \"\\^\", q))\n",
    "            hits = searcher.search(query, MAX)\n",
    "            doc_importance = [hit.score for hit in hits.scoreDocs]\n",
    "            scores.append(sum(doc_importance[:docs_per_q]))\n",
    "        guess = \"ABCD\"[np.argmax(scores)]\n",
    "        s.write(\"%s,%s\\n\" % (row[\"idd\"], guess))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Features\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURES_LUCENE_TOP100_SCORES = join(DATA_DIR, 'features/lucene_top100.tsv')\n",
    "FEATURES_LUCENE_ALL_SCORES = join(DATA_DIR, 'features/lucene_all.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 26.4 s, sys: 4.1 s, total: 30.5 s\n",
      "Wall time: 32.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "MAX = 100\n",
    "analyzer = StandardAnalyzer(Version.LUCENE_4_10_1)\n",
    "reader = IndexReader.open(SimpleFSDirectory(File(INDEX_DIR)))\n",
    "searcher = IndexSearcher(reader)\n",
    "\n",
    "with open(FEATURES_LUCENE_TOP100_SCORES, \"w\") as fs:\n",
    "    for row in iter_data(TRAINING_SET):\n",
    "        queries = [row['aa'], row['ab'], row['ac'], row['ad']]\n",
    "        queries = [row['q'] + ' ' + q  for q in queries]\n",
    "        features = []\n",
    "        for q in queries:\n",
    "            query = QueryParser(Version.LUCENE_4_10_1, \"text\", analyzer).parse(re.sub(\"[^a-zA-Z0-9]\",\" \", q))\n",
    "            hits = searcher.search(query, MAX)\n",
    "            doc_importances = [str(hit.score) for hit in hits.scoreDocs]\n",
    "            features.append(\";\".join(doc_importances))\n",
    "        print(row[\"idd\"], row[\"correct\"], *features, file=fs, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorter = Sort()\n",
    "sorter.setSort(SortField(\"content\", SortField.Type.STRING))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan']\n",
      "CPU times: user 22.3 ms, sys: 3.8 ms, total: 26.1 ms\n",
      "Wall time: 19.7 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "MAX = 10\n",
    "analyzer = StandardAnalyzer(Version.LUCENE_4_10_1)\n",
    "reader = IndexReader.open(SimpleFSDirectory(File(INDEX_DIR)))\n",
    "searcher = IndexSearcher(reader)\n",
    "\n",
    "with open(FEATURES_LUCENE_ALL_SCORES, \"w\") as fs:\n",
    "    for row in iter_data(TRAINING_SET):\n",
    "        queries = [row['aa'], row['ab'], row['ac'], row['ad']]\n",
    "        queries = [row['q'] + ' ' + q  for q in queries]\n",
    "        features = []\n",
    "        for q in queries:\n",
    "            query = QueryParser(Version.LUCENE_4_10_1, \"text\", analyzer).parse(re.sub(\"[^a-zA-Z0-9]\",\" \", q))      \n",
    "            hits = searcher.search(query, MAX, sorter)\n",
    "            doc_importances = [str(hit.score) for hit in hits.scoreDocs]\n",
    "            features.append(\";\".join(doc_importances))\n",
    "            print(doc_importances)\n",
    "            break\n",
    "        print(row[\"idd\"], row[\"correct\"], *features, file=fs, sep=\"\\t\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}