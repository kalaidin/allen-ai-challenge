{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, division, unicode_literals\n",
    "import six\n",
    "import os\n",
    "from os.path import join\n",
    "import json\n",
    "from codecs import open\n",
    "from collections import defaultdict\n",
    "from operator import itemgetter\n",
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import codecs\n",
    "\n",
    "import spacy\n",
    "\n",
    "from nltk.stem import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import spacy.en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords = set(stopwords.words('english')) + \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nlp = spacy.en.English()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    return [stemmer.stem(w) for w in nltk.word_tokenize(text.lower()) if w not in stopwords]\n",
    "#     return [stemmer.stem(w.text) for w in nlp(text) if not w.is_stop and not w.is_punct and not w.is_space]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import lucene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from org.apache.lucene.analysis.standard import StandardAnalyzer\n",
    "from org.apache.lucene.document import Document, Field\n",
    "from org.apache.lucene.index import IndexWriter, IndexWriterConfig, IndexReader\n",
    "from org.apache.lucene.search import IndexSearcher\n",
    "from org.apache.lucene.search import Sort, SortField\n",
    "from org.apache.lucene.queryparser.classic import QueryParser\n",
    "from org.apache.lucene.store import SimpleFSDirectory\n",
    "from org.apache.lucene.util import Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from java.io import File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DATA_DIR = join(os.environ['HOME'], 'data/allen-ai-challenge')\n",
    "WIKI_DIR = join(DATA_DIR, 'wiki_dump')\n",
    "CK12_DIR = join(DATA_DIR, 'ck12_dump')\n",
    "TRAINING_SET = join(DATA_DIR, 'training_set.tsv')\n",
    "VALIDATION_SET = join(DATA_DIR, 'validation_set.tsv')\n",
    "TRAINING_SET_MERGED = join(DATA_DIR, 'training_set_merged.tsv')\n",
    "INDEX_DIR = join(DATA_DIR, 'index-wiki-ck12')\n",
    "# INDEX_DIR = join(DATA_DIR, 'index-ck12-stem')\n",
    "INDEX_DIR = join(DATA_DIR, 'index-all-l_stem_summ')\n",
    "SUBMISSION = join(DATA_DIR, 'submissions/lucene_wiki_ck12_16jan.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<jcc.JCCEnv at 0x119a90570>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lucene.initVM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "analyzer = StandardAnalyzer(Version.LUCENE_4_10_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "writerConfig = IndexWriterConfig(Version.LUCENE_4_10_1, StandardAnalyzer())\n",
    "writer = IndexWriter(SimpleFSDirectory(File(INDEX_DIR)), writerConfig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 25 s, sys: 585 ms, total: 25.6 s\n",
      "Wall time: 26.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for i, fn_short in enumerate(os.listdir(CK12_DIR)):\n",
    "    fn = join(CK12_DIR, fn_short)\n",
    "    with open(fn, encoding='utf-8', errors='ignore') as f:\n",
    "        ck12_article = json.load(f)\n",
    "        content = ck12_article[\"science\"] + \" \" + ck12_article[\"concept\"] + \" \" + '. '.join([paragraph for subtitle, paragraph in ck12_article['contents'].items()])\n",
    "        content = \" \".join(tokenize(content))\n",
    "        doc = Document()\n",
    "        doc.add(Field(\"text\", content, Field.Store.YES, Field.Index.ANALYZED))\n",
    "        writer.addDocument(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# for i, fn_short in enumerate(os.listdir(WIKI_DIR)):\n",
    "#     fn = join(WIKI_DIR, fn_short)\n",
    "#     with open(fn, encoding='utf-8', errors='ignore') as f:\n",
    "#         wiki_article = json.load(f)\n",
    "#         a, b, content = wiki_article\n",
    "#         print(content)\n",
    "# #         content = \" \".join(tokenize(content[0]))\n",
    "#         if i > 2:\n",
    "#             break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6min 46s, sys: 6.97 s, total: 6min 53s\n",
      "Wall time: 7min 11s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for i, fn_short in enumerate(os.listdir(WIKI_DIR)):\n",
    "    fn = join(WIKI_DIR, fn_short)\n",
    "    with open(fn, encoding='utf-8', errors='ignore') as f:\n",
    "        wiki_article = json.load(f)\n",
    "        _, summary, content = wiki_article\n",
    "        content = summary + \" \" + content\n",
    "        content = \" \".join(tokenize(content))\n",
    "        doc = Document()\n",
    "        doc.add(Field(\"text\", content, Field.Store.YES, Field.Index.ANALYZED))\n",
    "        writer.addDocument(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import telepot\n",
    "def telegram_notify(msg):\n",
    "    token = \"178350795:AAFG7yae2SSt52GLek2bKS43oK7BaywWxRw\"\n",
    "    bot = telepot.Bot(token)\n",
    "    b = bot.getMe()\n",
    "    response = bot.getUpdates()\n",
    "    bot.sendMessage(31747780, msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "telegram_notify(\"lucene done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13635"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "writer.numDocs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def iter_data(datafile, with_correct=True):\n",
    "    with open(datafile, encoding='utf-8', errors='ignore') as f:\n",
    "        next(f)\n",
    "        for l in f:\n",
    "            if with_correct:\n",
    "                idd, q, correct, aa, ab, ac, ad = l.strip().split(\"\\t\")\n",
    "            else:\n",
    "                idd, q, aa, ab, ac, ad = l.strip().split(\"\\t\")\n",
    "                correct = \"no\"\n",
    "            q, aa, ab, ac, ad = [' '.join(tokenize(x)) for x in [q, aa, ab, ac, ad]]\n",
    "            yield {\"idd\": idd, \"q\": q, \"correct\": correct, \"aa\": aa, \"ab\": ab, \"ac\": ac, \"ad\": ad}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 17.3 s, sys: 1.78 s, total: 19.1 s\n",
      "Wall time: 19.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "res = defaultdict(list)\n",
    "MAX = 10\n",
    "docs_per_q = range(1, 20)\n",
    "\n",
    "analyzer = StandardAnalyzer(Version.LUCENE_4_10_1)\n",
    "reader = IndexReader.open(SimpleFSDirectory(File(INDEX_DIR)))\n",
    "searcher = IndexSearcher(reader)\n",
    "\n",
    "for row in iter_data(TRAINING_SET):\n",
    "    queries = [row['aa'], row['ab'], row['ac'], row['ad']]\n",
    "    queries = [row['q'] + ' ' + q  for q in queries]\n",
    "    scores = defaultdict(list)\n",
    "    for q in queries:\n",
    "        query = QueryParser(Version.LUCENE_4_10_1, \"text\", analyzer).parse(re.sub(\"[^a-zA-Z0-9]\",\" \", q))\n",
    "        #query = QueryParser(Version.LUCENE_30, \"text\", analyzer).parse(re.sub(\"[/^]\", \"\\^\", q))\n",
    "        hits = searcher.search(query, MAX)\n",
    "        doc_importance = [hit.score for hit in hits.scoreDocs]\n",
    "        for n in docs_per_q:\n",
    "            scores[n].append(sum(doc_importance[:n]))\n",
    "      \n",
    "    for n in docs_per_q:\n",
    "        res[n].append(['A','B','C','D'][np.argmax(scores[n])] == row[\"correct\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0.428\n",
      "2 0.43\n",
      "3 0.4328\n",
      "4 0.4332\n",
      "5 0.434\n",
      "6 0.4384\n",
      "7 0.4364\n",
      "8 0.4336\n",
      "9 0.4288\n"
     ]
    }
   ],
   "source": [
    "for x in sorted(res):\n",
    "    print(x, np.mean(res[x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 4s, sys: 7.01 s, total: 1min 11s\n",
      "Wall time: 1min 16s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "MAX = 100\n",
    "docs_per_q = 4\n",
    "\n",
    "analyzer = StandardAnalyzer(Version.LUCENE_4_10_1)\n",
    "reader = IndexReader.open(SimpleFSDirectory(File(INDEX_DIR)))\n",
    "searcher = IndexSearcher(reader)\n",
    "\n",
    "with open(SUBMISSION, \"w\") as s:\n",
    "    s.write(\"id,correctAnswer\\n\")\n",
    "    for row in iter_data(VALIDATION_SET, False):\n",
    "        queries = [row['aa'], row['ab'], row['ac'], row['ad']]\n",
    "        queries = [row['q'] + ' ' + q  for q in queries]\n",
    "        scores = []\n",
    "        for q in queries:\n",
    "            query = QueryParser(Version.LUCENE_4_10_1, \"text\", analyzer).parse(re.sub(\"[^a-zA-Z0-9]\",\" \", q))\n",
    "            #query = QueryParser(Version.LUCENE_30, \"text\", analyzer).parse(re.sub(\"[/^]\", \"\\^\", q))\n",
    "            hits = searcher.search(query, MAX)\n",
    "            doc_importance = [hit.score for hit in hits.scoreDocs]\n",
    "            scores.append(sum(doc_importance[:2]))\n",
    "        guess = \"ABCD\"[np.argmax(scores)]\n",
    "        s.write(\"%s,%s\\n\" % (row[\"idd\"], guess))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Features\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "FEATURES_LUCENE_TOP100_SCORES = join(DATA_DIR, 'features/lucene_top100.tsv')\n",
    "FEATURES_LUCENE_ALL_SCORES = join(DATA_DIR, 'features/lucene_all.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 26.4 s, sys: 4.1 s, total: 30.5 s\n",
      "Wall time: 32.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "MAX = 100\n",
    "analyzer = StandardAnalyzer(Version.LUCENE_4_10_1)\n",
    "reader = IndexReader.open(SimpleFSDirectory(File(INDEX_DIR)))\n",
    "searcher = IndexSearcher(reader)\n",
    "\n",
    "with open(FEATURES_LUCENE_TOP100_SCORES, \"w\") as fs:\n",
    "    for row in iter_data(TRAINING_SET):\n",
    "        queries = [row['aa'], row['ab'], row['ac'], row['ad']]\n",
    "        queries = [row['q'] + ' ' + q  for q in queries]\n",
    "        features = []\n",
    "        for q in queries:\n",
    "            query = QueryParser(Version.LUCENE_4_10_1, \"text\", analyzer).parse(re.sub(\"[^a-zA-Z0-9]\",\" \", q))\n",
    "            hits = searcher.search(query, MAX)\n",
    "            doc_importances = [str(hit.score) for hit in hits.scoreDocs]\n",
    "            features.append(\";\".join(doc_importances))\n",
    "        print(row[\"idd\"], row[\"correct\"], *features, file=fs, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sorter = Sort()\n",
    "sorter.setSort(SortField(\"content\", SortField.Type.STRING))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan']\n",
      "CPU times: user 22.3 ms, sys: 3.8 ms, total: 26.1 ms\n",
      "Wall time: 19.7 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "MAX = 10\n",
    "analyzer = StandardAnalyzer(Version.LUCENE_4_10_1)\n",
    "reader = IndexReader.open(SimpleFSDirectory(File(INDEX_DIR)))\n",
    "searcher = IndexSearcher(reader)\n",
    "\n",
    "with open(FEATURES_LUCENE_ALL_SCORES, \"w\") as fs:\n",
    "    for row in iter_data(TRAINING_SET):\n",
    "        queries = [row['aa'], row['ab'], row['ac'], row['ad']]\n",
    "        queries = [row['q'] + ' ' + q  for q in queries]\n",
    "        features = []\n",
    "        for q in queries:\n",
    "            query = QueryParser(Version.LUCENE_4_10_1, \"text\", analyzer).parse(re.sub(\"[^a-zA-Z0-9]\",\" \", q))      \n",
    "            hits = searcher.search(query, MAX, sorter)\n",
    "            doc_importances = [str(hit.score) for hit in hits.scoreDocs]\n",
    "            features.append(\";\".join(doc_importances))\n",
    "            print(doc_importances)\n",
    "            break\n",
    "        print(row[\"idd\"], row[\"correct\"], *features, file=fs, sep=\"\\t\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
