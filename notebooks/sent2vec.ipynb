{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division, unicode_literals\n",
    "import numpy as np\n",
    "import codecs\n",
    "import random\n",
    "import nltk\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 8s, sys: 1.08 s, total: 1min 9s\n",
      "Wall time: 1min 10s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from spacy.en import English\n",
    "nlp = English()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In -> weathered  [u'order'] prep IN [In , order , for , boulders , to , become , sediments]\n",
      "order -> In  [u'become'] pobj NN [order , for , boulders , to , become , sediments]\n",
      "for -> become  [] mark IN [for ]\n",
      "boulders -> become  [] nsubj NNS [boulders ]\n",
      "to -> become  [] aux TO [to ]\n",
      "become -> order  [u'for', u'boulders', u'to', u'sediments'] advcl VB [for , boulders , to , become , sediments]\n",
      "sediments -> become  [] attr NNS [sediments]\n",
      ", -> weathered  [] punct , [, ]\n",
      "the -> boulders  [] det DT [the ]\n",
      "boulders -> weathered  [u'the'] nsubjpass NNS [the , boulders ]\n",
      "must -> weathered  [] aux MD [must ]\n",
      "first -> weathered  [] advmod RB [first ]\n",
      "be -> weathered  [] auxpass VB [be ]\n",
      "weathered -> weathered  [u'In', u',', u'boulders', u'must', u'first', u'be', u'and', u'eroded', u'.'] ROOT VBN [In , order , for , boulders , to , become , sediments, , , the , boulders , must , first , be , weathered , and , eroded, .]\n",
      "and -> weathered  [] cc CC [and ]\n",
      "eroded -> weathered  [] conj VBN [eroded]\n",
      ". -> weathered  [] punct . [.]\n"
     ]
    }
   ],
   "source": [
    "for w in nlp('In order for boulders to become sediments, the boulders must first be weathered and eroded.'):\n",
    "    print(w.text, '->', w.head, [c.text for c in w.children], w.dep_, w.tag_, [s for s in w.subtree])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA = \"/Users/Pavel/Code/allen-ai-challenge/data/ck12_tokens.txt\"\n",
    "DATA = \"/home/marat/ck12_tokens.txt\"\n",
    "TRAINING_SET_MERGED = \"/home/marat/Downloads/training_set_merged.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "GLOVE_D = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "GLOVE_FILE = \"/home/marat/data/glove.6B/glove.6B.%dd.txt\" % GLOVE_D\n",
    "GLOVE_FILE = \"/home/marat/data/word2vec.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dict(fname):\n",
    "    d = {}\n",
    "    with codecs.open(fname, encoding=\"utf-8\") as f:\n",
    "        for row in (line.strip().split() for line in f):\n",
    "            w = row[0]\n",
    "            v = np.array(row[1:], dtype='float32')\n",
    "            assert v.shape == (GLOVE_D,)\n",
    "            d[w] = v\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GLOVE = read_dict(GLOVE_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(GLOVE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def text2vec(text, glove_dict, seq_length, debug=False):\n",
    "#     vecs = []\n",
    "#     for w in text.split():\n",
    "#         try:\n",
    "#             vecs.append(glove_dict[w][np.newaxis, :])\n",
    "#         except KeyError:\n",
    "#             if debug:\n",
    "#                 print(\"%s is not found\" % w)\n",
    "#             continue\n",
    "#     if not vecs:\n",
    "#         print(\"no vector for '%s'\" % text)\n",
    "#         return np.zeros((seq_length, GLOVE_D))\n",
    "    \n",
    "#     rec = np.concatenate(vecs, axis=0).astype('float32')\n",
    "#     if rec.shape[0] > seq_length:\n",
    "#         # trim long sentences\n",
    "#         rec = rec[rec.shape[0] - seq_length:, :]\n",
    "#     elif rec.shape[0] < seq_length:\n",
    "#         # extend short sentences with zeros\n",
    "#         rec = np.vstack([np.zeros((seq_length - rec.shape[0], rec.shape[1])), rec])\n",
    "#     assert rec.shape[0] == seq_length\n",
    "#     return rec\n",
    "\n",
    "def text2vec(text, seq_length, debug=False):\n",
    "    D = 300\n",
    "    vecs = []\n",
    "    for w in text:\n",
    "        vecs.append(w.vector[np.newaxis])\n",
    "\n",
    "    if not vecs:\n",
    "        print(\"no vector for '%s'\" % text)\n",
    "        return np.zeros((seq_length, D), dtype='float32')\n",
    "    \n",
    "    rec = np.concatenate(vecs, axis=0)\n",
    "    if rec.shape[0] > seq_length:\n",
    "        # trim long sentences\n",
    "        rec = rec[rec.shape[0] - seq_length:, :]\n",
    "    elif rec.shape[0] < seq_length:\n",
    "        # extend short sentences with zeros\n",
    "        rec = np.vstack([np.zeros((seq_length - rec.shape[0], rec.shape[1])), rec])\n",
    "    assert rec.shape[0] == seq_length\n",
    "    return rec.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text2vec(next(nlp(\"hello , die\").sents), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# i=0\n",
    "# sentences = []\n",
    "# with codecs.open(DATA, encoding=\"utf-8\") as f:\n",
    "#     for line in f:\n",
    "#         if u'↓' in line:\n",
    "#             continue\n",
    "# #         i += 1\n",
    "# #         if 63798 < i 63800 :\n",
    "#         sentences.append(line.strip())\n",
    "        \n",
    "# blob = TextBlob(' '.join(sentences))\n",
    "# all_pos_tags = blob.tags        \n",
    "\n",
    "# i = 0\n",
    "# pos_tags = []\n",
    "# for l, line in enumerate(sentences):    \n",
    "#     tags = []\n",
    "#     for word in line.split():\n",
    "# #         if l == 63798:\n",
    "# #         print(word, i, all_pos_tags[i])\n",
    "#         if word.replace(\".\", \"\") == all_pos_tags[i][0].replace(\".\", \"\"):\n",
    "#             tags.append(all_pos_tags[i][1])\n",
    "#             i += 1\n",
    "#         else:\n",
    "#             tags.append('.')\n",
    "            \n",
    "#     pos_tags.append(tags)\n",
    "# #     if l == 63798:\n",
    "# #     print(line, tags)\n",
    "#     break\n",
    "\n",
    "# del all_pos_tags\n",
    "\n",
    "# assert len(sentences) == len(pos_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 42s, sys: 416 ms, total: 1min 42s\n",
      "Wall time: 1min 42s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sentences = []\n",
    "with codecs.open(DATA, encoding=\"utf-8\") as f:\n",
    "    for i, line in enumerate(f):\n",
    "        if u'↓' in line:\n",
    "            continue\n",
    "        for s in nlp(line.strip()).sents:\n",
    "            sentences.append(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from spacy import parts_of_speech as pos\n",
    "# from spacy.util import DEP\n",
    "# # print(sentences[0])\n",
    "# for i, s in enumerate(sentences[10000:]):\n",
    "#     if i >= 100:\n",
    "#         break\n",
    "# #     if i != 7:\n",
    "# #         continue\n",
    "#     has_conj = False\n",
    "#     for w in s:\n",
    "#         if 'conj' in w.dep_:\n",
    "#             has_conj = True\n",
    "#             break\n",
    "#     if not has_conj:\n",
    "#         continue\n",
    "    \n",
    "\n",
    "#     print(s.text)\n",
    "# #     s = next(nlp(s).sents)\n",
    "\n",
    "#     subj = None\n",
    "#     for w in s:\n",
    "#         if 'nsubj' in w.dep_:\n",
    "#             subj = w\n",
    "# #         print('\\t', w.text, w.dep_, w.tag_)\n",
    "\n",
    "#     for w in s:\n",
    "#         if 'conj' in w.dep_:\n",
    "#             print(w, w.tag_, '&', w.head, w.head.tag_, '->', w.head.head, w.head.head.tag_)\n",
    "\n",
    "#     for n in s.doc.noun_chunks:\n",
    "#         print('*', n)\n",
    "        \n",
    "\n",
    "#     print(subj, s.root)\n",
    "#     print('-'*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CC Coordinating conjunction\n",
    "CD Cardinal number\n",
    "DT Determiner\n",
    "EX Existential there\n",
    "FW Foreign word\n",
    "IN Preposition or subordinating conjunction\n",
    "JJ Adjective\n",
    "JJR Adjective, comparative\n",
    "JJS Adjective, superlative\n",
    "LS List item marker\n",
    "MD Modal\n",
    "NN Noun, singular or mass\n",
    "NNS Noun, plural\n",
    "NNP Proper noun, singular\n",
    "NNPS Proper noun, plural\n",
    "PDT Predeterminer\n",
    "POS Possessive ending\n",
    "PRP Personal pronoun\n",
    "PRP$ Possessive pronoun\n",
    "RB Adverb\n",
    "RBR Adverb, comparative\n",
    "RBS Adverb, superlative\n",
    "RP Particle\n",
    "SYM Symbol\n",
    "TO to\n",
    "UH Interjection\n",
    "VB Verb, base form\n",
    "VBD Verb, past tense\n",
    "VBG Verb, gerund or present participle\n",
    "VBN Verb, past participle\n",
    "VBP Verb, non­3rd person singular present\n",
    "VBZ Verb, 3rd person singular present\n",
    "WDT Wh­determiner\n",
    "WP Wh­pronoun\n",
    "WP$ Possessive wh­pronoun\n",
    "WRB Wh­adverb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "CORRUPT_WINDOW = 10\n",
    "# def corrupt(sentences, index):\n",
    "#     s = sentences[index]\n",
    "#     noun_indices = [i for i, w in enumerate(s) if 'NN' in w.tag_]\n",
    "#     if not noun_indices:\n",
    "#         return None\n",
    "#     noun_to_replace_index = random.choice(noun_indices)\n",
    "#     all_donor_indices = range(max(0, index-CORRUPT_WINDOW), index) + \\\n",
    "#                         range(index+1, min(len(sentences), index+CORRUPT_WINDOW+1))\n",
    "#     donor_index = random.choice(all_donor_indices)\n",
    "#     all_nouns_to_insert = [w for w in sentences[donor_index] if 'NN' in w.tag_]\n",
    "#     if not all_nouns_to_insert:\n",
    "#         return None\n",
    "#     noun_to_insert = random.choice(all_nouns_to_insert)\n",
    "    \n",
    "#     return next(nlp(s[:noun_to_replace_index].text + ' ' + noun_to_insert.text + ' ' \n",
    "#                     + s[noun_to_replace_index+1:].text).sents)\n",
    "\n",
    "def corrupt(sentences, index):\n",
    "    s = sentences[index]\n",
    "    noun_chunks = list(s.doc.noun_chunks)\n",
    "    if not noun_chunks:\n",
    "        return None\n",
    "    noun_chunk_to_replace = random.choice(noun_chunks)\n",
    "    \n",
    "    all_donor_indices = range(max(0, index-CORRUPT_WINDOW), index) + \\\n",
    "                        range(index+1, min(len(sentences), index+CORRUPT_WINDOW+1))\n",
    "    donor_index = random.choice(all_donor_indices)\n",
    "    all_noun_chunks_to_insert = list(sentences[donor_index].doc.noun_chunks)\n",
    "    if not all_noun_chunks_to_insert:\n",
    "        return None\n",
    "    np_to_insert = random.choice(all_noun_chunks_to_insert)\n",
    "    \n",
    "    new_text = s[:noun_chunk_to_replace.start].text + ' ' + np_to_insert.text + ' ' \\\n",
    "                    + s[noun_chunk_to_replace.end:].text\n",
    "    \n",
    "    return next(nlp(new_text).sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# for c in range(len(sentences)):\n",
    "#     corrupted = corrupt(sentences, c)\n",
    "#     if corrupted:\n",
    "#         text2vec(sentences[c], SEQ_LENGTH), text2vec(corrupted, SEQ_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling functions ...\n",
      "CPU times: user 2.47 s, sys: 260 ms, total: 2.73 s\n",
      "Wall time: 10.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import lasagne\n",
    "from lasagne.nonlinearities import very_leaky_rectify, leaky_rectify\n",
    "theano.config.floatX = 'float32'\n",
    "\n",
    "SEQ_LENGTH = 30\n",
    "MARGIN = 1\n",
    "\n",
    "l_in = lasagne.layers.InputLayer(shape=(None, GLOVE_D, SEQ_LENGTH))\n",
    "n = lasagne.layers.GRULayer()\n",
    "# n = lasagne.layers.Conv1DLayer(l_in, 30, filter_size=3, nonlinearity=very_leaky_rectify)  # None x 10 x 28\n",
    "# n = lasagne.layers.MaxPool1DLayer(n, 2)  # None x f x 14\n",
    "# n = lasagne.layers.Conv1DLayer(l_in, 40, filter_size=3, nonlinearity=very_leaky_rectify)  # None x 20 x 12\n",
    "# n = lasagne.layers.MaxPool1DLayer(n, 2)  # None x f x 6\n",
    "# n = lasagne.layers.Conv1DLayer(l_in, 50, filter_size=3, nonlinearity=leaky_rectify)  # None x 30 x 4\n",
    "# n = lasagne.layers.MaxPool1DLayer(n, 2)  # None x f x 2\n",
    "n = lasagne.layers.reshape(n, ([0], -1))\n",
    "# n = lasagne.layers.DropoutLayer(n, 0.5)\n",
    "n = lasagne.layers.DenseLayer(n, 100)\n",
    "n = lasagne.layers.DenseLayer(n, 1)\n",
    "\n",
    "output = lasagne.layers.get_output(n)\n",
    "params = lasagne.layers.get_all_params(n)\n",
    "\n",
    "correct_energy = output[0::2][0]  # 50\n",
    "corrupt_energy = output[1::2][0]  # 50\n",
    "\n",
    "# energy = T.maximum(0, MARGIN + correct_energy - corrupt_energy).mean()  # hinge loss\n",
    "# energy = (correct_energy**2).mean() + ((T.maximum(0, MARGIN - corrupt_energy))**2).mean()  # square-square loss\n",
    "energy = (correct_energy**2).mean() + (MARGIN * T.exp(-corrupt_energy)).mean()  # square-exponential\n",
    "updates = lasagne.updates.adam(energy, params)\n",
    "\n",
    "print('Compiling functions ...')\n",
    "forward_fn = theano.function([l_in.input_var], output)\n",
    "train_fn = theano.function([l_in.input_var], energy, updates=updates)\n",
    "test_fn = theano.function([l_in.input_var], energy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def energy_fn(txt):\n",
    "    sents = nlp(txt).sents\n",
    "    data = [text2vec(s, SEQ_LENGTH).T[np.newaxis] for s in sents]\n",
    "    return forward_fn(np.concatenate(data, axis=0)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.832509 24.3% 326s\n",
      "1 0.828645 25.1% 356s\n",
      "2 0.827393 24.1% 322s\n",
      "3 0.826316 25.6% 330s\n",
      "4 0.824849 26.1% 325s\n",
      "5 0.825799 25.0% 325s\n",
      "6 0.82316 24.4% 322s\n",
      "7 0.823376 24.0% 322s\n",
      "8 0.820097 23.4% 322s\n",
      "9 0.825687 24.7% 324s\n",
      "10 0.818421 23.7% 323s\n",
      "11 0.822294 25.0% 322s\n",
      "12 0.824571 23.9% 321s\n",
      "13 0.825725 24.8% 327s\n",
      "14 0.820987 25.8% 327s\n",
      "15 0.820714 24.9% 331s\n",
      "16 0.819233 24.7% 321s\n",
      "17 0.823632 23.9% 321s\n",
      "18 0.822845 24.1% 323s\n",
      "19 0.819962 25.6% 325s\n",
      "20 0.817265 25.0% 325s\n",
      "21 0.816961 24.3% 324s\n",
      "22 0.816117 23.7% 320s\n",
      "23 0.837103 24.9% 325s\n",
      "24 0.823386 24.4% 324s\n",
      "25 0.81867 24.9% 326s\n",
      "26 0.817251 25.6% 322s\n",
      "27 0.817315 25.7% 323s\n",
      "28 0.818032 23.2% 324s\n",
      "29 0.8147 25.2% 325s\n",
      "30 0.819606 24.1% 325s\n",
      "31 0.815127 23.5% 325s\n",
      "32 0.806638 24.8% 325s\n",
      "33 0.815123 24.0% 322s\n",
      "34 0.813973 23.0% 320s\n",
      "35 0.814883 22.4% 320s\n",
      "36 0.811355 24.2% 321s\n",
      "37 0.809877 23.8% 321s\n",
      "38 0.808651 23.8% 323s\n",
      "39 0.806798 24.2% 321s\n",
      "40 0.804566 23.8% 322s\n",
      "41 0.816157 23.8% 321s\n",
      "42 0.803599 23.3% 322s\n",
      "43 0.804702 23.6% 323s\n",
      "44 0.802262 23.8% 318s\n",
      "45 0.811755 22.7% 317s\n",
      "46 0.796903 23.4% 317s\n",
      "47 0.862422 24.6% 320s\n",
      "48 0.795102 23.8% 319s\n",
      "49 0.808592 24.5% 322s\n",
      "50 0.804619 23.6% 324s\n",
      "51 0.800881 24.4% 321s\n",
      "52 0.795793 24.6% 318s\n",
      "53 0.798846 24.3% 320s\n",
      "54 0.802918 23.1% 320s\n",
      "55 0.803252 24.8% 321s\n",
      "56 0.792357 25.2% 317s\n",
      "57 0.808525 24.6% 324s\n",
      "58 0.802033 24.5% 325s\n",
      "59 0.793588 25.4% 324s\n",
      "60 0.79443 24.5% 321s\n",
      "61 0.798625 24.5% 322s\n",
      "62 0.831828 24.0% 321s\n",
      "63 0.792297 25.9% 321s\n",
      "64 0.80224 24.0% 324s\n",
      "65 0.850676 25.1% 326s\n",
      "66 0.796003 23.8% 325s\n",
      "67 0.791736 24.4% 318s\n",
      "68 0.795265 25.7% 319s\n",
      "69 0.837313 25.1% 318s\n",
      "70 0.798692 24.1% 320s\n",
      "71 0.790403 24.5% 322s\n",
      "72 0.794615 23.8% 321s\n",
      "73 0.791051 25.0% 323s\n",
      "74 0.781766 25.3% 321s\n",
      "75 0.81935 23.6% 323s\n",
      "76 0.805722 24.4% 322s\n",
      "77 0.792176 24.6% 323s\n",
      "78 0.789208 23.6% 320s\n",
      "79 0.80665 24.9% 324s\n",
      "80 0.795286 22.4% 323s\n",
      "81 0.787699 24.8% 323s\n",
      "82 0.790897 24.1% 320s\n",
      "83 0.817718 24.5% 320s\n",
      "84 0.80947 25.0% 320s\n",
      "85 0.796032 24.5% 324s\n",
      "86 0.812319 23.5% 324s\n",
      "87 0.793919 23.9% 323s\n",
      "88 0.783826 23.5% 321s\n",
      "89 0.795815 23.6% 319s\n",
      "90 0.805126 25.2% 321s\n",
      "91 0.793391 24.6% 322s\n",
      "92 0.800727 24.7% 321s\n",
      "93 0.78805 25.4% 317s\n",
      "94 0.799951 25.9% 322s\n",
      "95 0.798465 26.2% 325s\n",
      "96 0.790938 25.6% 319s\n",
      "97 0.787959 24.2% 318s\n",
      "98 0.793309 23.9% 319s\n",
      "99 0.781897 26.2% 318s\n",
      "CPU times: user 15h 10min 5s, sys: 20h 18min 5s, total: 1d 11h 28min 10s\n",
      "Wall time: 8h 57min 15s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "BATCH_SIZE = 50\n",
    "EPOCH_COUNT = 100\n",
    "indices = np.arange(len(sentences))\n",
    "for e in range(EPOCH_COUNT):\n",
    "    epoch_start = time()\n",
    "    \n",
    "    tries = []\n",
    "    with codecs.open(TRAINING_SET_MERGED, encoding=\"utf-8\") as f:\n",
    "        for i, l in enumerate(f):\n",
    "            q_id, correct, a1, a2, a3, a4 = l.strip().split(\"\\t\")\n",
    "            energies = [energy_fn(v) for v in [a1, a2, a3, a4]]\n",
    "            guess = \"ABCD\"[np.argmin(energies)]\n",
    "            tries.append(guess == correct)   \n",
    "    \n",
    "    np.random.shuffle(indices)\n",
    "    errors = []\n",
    "    for i in xrange(0, indices.shape[0], BATCH_SIZE):\n",
    "        train_sent_idx = [k for k in indices[i:i+BATCH_SIZE]]\n",
    "        train_data = []\n",
    "        for correct_idx in train_sent_idx:\n",
    "            corrupted = corrupt(sentences, correct_idx)\n",
    "            if corrupted:\n",
    "                train_data.append(text2vec(sentences[correct_idx], SEQ_LENGTH).T[np.newaxis])\n",
    "                train_data.append(text2vec(corrupted, SEQ_LENGTH).T[np.newaxis])\n",
    "        train_data = np.concatenate(train_data, axis=0)\n",
    "        error = train_fn(train_data)\n",
    "        errors.append(error)\n",
    "                   \n",
    "    time_passed = time() - epoch_start\n",
    "    print(e, np.mean(errors), '%.1f%%' % (np.mean(tries) * 100),'%.0fs' % time_passed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v1 = 'the sun is the main source of energy for the water cycle .'\n",
    "v2 = 'fossil fuels is the main source of energy for the water cycle .'\n",
    "v3 = 'clouds is the main source of energy for the water cycle .'\n",
    "v4 = 'the ocean is the main source of energy for the water cycle .'\n",
    "\n",
    "# v1 = 'tension has the greatest effect on aiding the movement of blood through the human body .'\n",
    "# v2 = 'friction has the greatest effect on aiding the movement of blood through the human body .'\n",
    "# v3 = 'density has the greatest effect on aiding the movement of blood through the human body .'\n",
    "# v4 = 'gravity has the greatest effect on aiding the movement of blood through the human body .'\n",
    "\n",
    "print(energy_fn(v1))\n",
    "print(energy_fn(v2))\n",
    "print(energy_fn(v3))\n",
    "print(energy_fn(v4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "tries = []\n",
    "\n",
    "with codecs.open(TRAINING_SET_MERGED, encoding=\"utf-8\") as f:\n",
    "    for i, l in enumerate(f):\n",
    "        q_id, correct, a1, a2, a3, a4 = l.strip().split(\"\\t\")\n",
    "        energies = [energy_fn(v) for v in [a1, a2, a3, a4]]\n",
    "        guess = \"ABCD\"[np.argmin(energies)]\n",
    "#         print(guess, correct, q_id, zip([a1, a2, a3, a4], energies))\n",
    "        tries.append(guess == correct)\n",
    "#         if i > 10:\n",
    "#             break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(tries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}