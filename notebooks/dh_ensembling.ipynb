{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, division, unicode_literals\n",
    "import six\n",
    "import os\n",
    "from os.path import join\n",
    "import json\n",
    "from codecs import open\n",
    "from collections import defaultdict\n",
    "from operator import itemgetter\n",
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import codecs\n",
    "import pandas as pd\n",
    "import random\n",
    "from itertools import islice\n",
    "\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import lasagne\n",
    "import lasagne.layers as LL\n",
    "from lasagne.nonlinearities import softmax, elu\n",
    "\n",
    "from sklearn.cross_validation import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATA_DIR = join(os.environ['HOME'], 'data/allen-ai-challenge')\n",
    "FEATURE_DIR = join(DATA_DIR, 'features')\n",
    "\n",
    "TRAINING_CORRECTS = join(FEATURE_DIR, 'correct_answers.tsv')\n",
    "TRAINING_SET = join(DATA_DIR, 'training_set.tsv')\n",
    "VALIDATION_SET = join(DATA_DIR, 'validation_set.tsv')\n",
    "\n",
    "SUMBISSION_FILE = join(DATA_DIR, 'submissions', 'ensemble1.csv')\n",
    "\n",
    "# DIM_NGRAMS = 1\n",
    "# NGRAMS = 'ngrams_0429_1483'\n",
    "\n",
    "DIM_NGRAMS = 4\n",
    "NGRAMS = 'merged_ngrams'\n",
    "\n",
    "DIM_REPTIL = 1\n",
    "REPTIL = 'set_reptil_features'\n",
    "\n",
    "DIM_LUCENE_ROMAN = 50\n",
    "LUCENE_ROMAN = 'lucene_f5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fn(dataset_name, dataset_type='training'):\n",
    "    assert dataset_type in ['training', 'validation']\n",
    "    return join(FEATURE_DIR, ('%s_%s.tsv' % (dataset_type, dataset_name)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(TRAINING_CORRECTS):\n",
    "    print(TRAINING_CORRECTS, 'not found. Creating a new one...')\n",
    "    with open(TRAINING_SET, encoding='utf8') as fi:\n",
    "        fi.readline()  # skip header\n",
    "        with open(TRAINING_CORRECTS, encoding='utf8', mode='w') as fo:\n",
    "            for line in fi:\n",
    "                qid = line.split('\\t')[0]\n",
    "                correct = line.split('\\t')[2]\n",
    "                print(qid, correct, sep='\\t', file=fo)      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data\n",
    "-------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "correct_map = {}\n",
    "with open(TRAINING_CORRECTS, encoding='utf8') as f:\n",
    "    for qid, c in (line.strip().split() for line in f):\n",
    "        correct_map[int(qid)] = 'ABCD'.index(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "idx_valid = []\n",
    "with open(VALIDATION_SET, encoding='utf8') as f:\n",
    "    f.readline()\n",
    "    for row in (line.strip().split('\\t') for line in f):\n",
    "        idx_valid.append(int(row[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ids = sorted(correct_map)\n",
    "itrain, itest = next(iter(KFold(len(ids), n_folds=3)))\n",
    "idx_train = [ids[i] for i in itrain]\n",
    "idx_test = [ids[i] for i in itest]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cycle_rand(seq):\n",
    "    while True:\n",
    "        rseq = list(seq)\n",
    "        random.shuffle(rseq)\n",
    "        for s in rseq:\n",
    "            yield s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def take(n, iterable):\n",
    "    \"Return first n items of the iterable as a list\"\n",
    "    return list(islice(iterable, n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_dataset_(filename, dims, dict_out):\n",
    "    with open(filename, encoding='utf8') as f:\n",
    "        for row in (line.strip().split('\\t') for line in f):\n",
    "            qid = int(row[0])\n",
    "            dict_out[qid] = np.array([np.fromstring(x, sep=';') for x in row[1:]])\n",
    "\n",
    "            \n",
    "def read_dataset(dataset_name, dims=1):\n",
    "    data = defaultdict(lambda: np.zeros((4, dims)))\n",
    "    read_dataset_(fn(dataset_name, 'training'), dims, data)\n",
    "    read_dataset_(fn(dataset_name, 'validation'), dims, data)\n",
    "    return data\n",
    "\n",
    "\n",
    "def generate_data(indices, data_x_dict):\n",
    "    f_dim = data_x_dict.values()[0][0].shape[0]\n",
    "    \n",
    "    x = np.zeros((len(indices), 4, f_dim), dtype='float32')\n",
    "    y = np.zeros((len(indices)), dtype='int32')\n",
    "    for row, i in enumerate(indices):\n",
    "        x[row] = data_x_dict[i]\n",
    "        y[row] = correct_map.get(i, 0)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def shuffle_dataset(dataset_x, dataset_y):\n",
    "    f_dim = dataset_x.shape[2]  # feature dimensions\n",
    "    rx = np.zeros_like(dataset_x)\n",
    "    ry = np.zeros_like(dataset_y)\n",
    "    for i_row in xrange(dataset_y.shape[0]):\n",
    "        rand_order = np.arange(4)\n",
    "        np.random.shuffle(rand_order)\n",
    "        for k in range(4):\n",
    "            ri = rand_order[k]\n",
    "            rx[i_row, ri] = dataset_x[i_row, k]\n",
    "        ry[i_row] = rand_order[dataset_y[i_row]]\n",
    "    return rx, ry\n",
    "\n",
    "\n",
    "def flatten_dataset(dataset_x, dataset_y):\n",
    "    row_count, choice_count, f_dim = dataset_x.shape  # feature dimensions\n",
    "    return dataset_x.reshape(row_count, choice_count * f_dim), dataset_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# generate_data([100001], lucene)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# shuffle_dataset(*generate_data([100001], lucene))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NGrams\n",
    "-----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ngrams = read_dataset(NGRAMS, dims=DIM_NGRAMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "L2 = 0.0001\n",
    "\n",
    "t_target = T.ivector()\n",
    "\n",
    "l_in = LL.InputLayer((None, 4, DIM_NGRAMS))\n",
    "nn = LL.DenseLayer(l_in, 30, nonlinearity=elu)\n",
    "nn = LL.DenseLayer(nn, 4, nonlinearity=softmax, b=None)\n",
    "t_output = LL.get_output(nn)\n",
    "\n",
    "t_cost = lasagne.objectives.categorical_crossentropy(t_output, t_target).mean()\n",
    "t_acc = lasagne.objectives.categorical_accuracy(t_output, t_target).mean()\n",
    "\n",
    "params = LL.get_all_params(nn)\n",
    "\n",
    "l2 = lasagne.regularization.regularize_network_params(nn, lasagne.regularization.l2)\n",
    "\n",
    "updates = lasagne.updates.adam(t_cost + L2*l2, params)\n",
    "\n",
    "train_fn = theano.function([l_in.input_var, t_target], t_cost, updates=updates)\n",
    "cost_fn = theano.function([l_in.input_var, t_target], [t_cost, t_acc])\n",
    "\n",
    "forward_fn = theano.function([l_in.input_var], t_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "id_generator = cycle_rand(idx_train)\n",
    "\n",
    "test_x, test_y = generate_data(idx_test, ngrams)\n",
    "\n",
    "BATCH = 100\n",
    "costs = []\n",
    "rows_seen = 0\n",
    "while rows_seen < 500000:\n",
    "    indices = take(BATCH, id_generator)\n",
    "    batch_x, batch_y = shuffle_dataset(*generate_data(indices, ngrams))\n",
    "    costs.append(train_fn(batch_x, batch_y))\n",
    "    rows_seen += BATCH\n",
    "    if rows_seen % 10000 < BATCH:\n",
    "        nll, acc = cost_fn(test_x, test_y)\n",
    "        mean_acc = (test_x.sum(axis=2).argmax(axis=1) == test_y).sum() / test_y.shape[0]\n",
    "        print(rows_seen, np.mean(costs), nll, acc, mean_acc)\n",
    "        costs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ngrams_train_pred = forward_fn(generate_data(idx_train, ngrams)[0])\n",
    "ngrams_test_pred = forward_fn(generate_data(idx_test, ngrams)[0])\n",
    "ngrams_valid_pred = forward_fn(generate_data(idx_valid, ngrams)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "REPTIL\n",
    "--------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reptil = read_dataset(REPTIL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t_target = T.ivector()\n",
    "\n",
    "l_in = LL.InputLayer((None, 4, 1))\n",
    "nn = LL.DenseLayer(l_in, 30, nonlinearity=elu)\n",
    "nn = LL.DenseLayer(nn, 4, nonlinearity=softmax, b=None)\n",
    "t_output = LL.get_output(nn, deterministic=False)\n",
    "t_output_det = LL.get_output(nn, deterministic=True)\n",
    "\n",
    "t_cost = lasagne.objectives.categorical_crossentropy(t_output, t_target).mean()\n",
    "t_acc = lasagne.objectives.categorical_accuracy(t_output, t_target).mean()\n",
    "\n",
    "t_cost_det = lasagne.objectives.categorical_crossentropy(t_output_det, t_target).mean()\n",
    "t_acc_det = lasagne.objectives.categorical_accuracy(t_output_det, t_target).mean()\n",
    "\n",
    "params = LL.get_all_params(nn)\n",
    "\n",
    "l2 = lasagne.regularization.regularize_network_params(nn, lasagne.regularization.l2)\n",
    "updates = lasagne.updates.adam(t_cost + L2*l2, params)\n",
    "\n",
    "train_fn = theano.function([l_in.input_var, t_target], t_cost, updates=updates)\n",
    "cost_fn = theano.function([l_in.input_var, t_target], [t_cost_det, t_acc_det])\n",
    "\n",
    "forward_fn = theano.function([l_in.input_var], t_output_det)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "id_generator = cycle_rand(idx_train)\n",
    "\n",
    "test_x, test_y = generate_data(idx_test, reptil)\n",
    "\n",
    "BATCH = 100\n",
    "costs = []\n",
    "rows_seen = 0\n",
    "while rows_seen < 200000:\n",
    "    indices = take(BATCH, id_generator)\n",
    "    batch_x, batch_y = shuffle_dataset(*generate_data(indices, reptil))\n",
    "    costs.append(train_fn(batch_x, batch_y))\n",
    "    rows_seen += BATCH\n",
    "    if rows_seen % 1000 < BATCH:\n",
    "        nll, acc = cost_fn(test_x, test_y)\n",
    "        mean_acc = (test_x[:,:,0].argmax(axis=1) == test_y).sum() / test_y.shape[0]\n",
    "        print(rows_seen, np.mean(costs), nll, acc, mean_acc)\n",
    "        costs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reptil_train_pred = forward_fn(generate_data(idx_train, reptil)[0])\n",
    "reptil_test_pred = forward_fn(generate_data(idx_test, reptil)[0])\n",
    "reptil_valid_pred = forward_fn(generate_data(idx_valid, reptil)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lucene + Roman\n",
    "-------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lucene_roman = read_dataset(LUCENE_ROMAN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t_target = T.ivector()\n",
    "\n",
    "l_in = LL.InputLayer((None, 4, 50))\n",
    "nn = LL.DenseLayer(l_in, 300, nonlinearity=elu)\n",
    "nn = LL.DenseLayer(nn, 4, nonlinearity=softmax, b=None)\n",
    "t_output = LL.get_output(nn, deterministic=False)\n",
    "t_output_det = LL.get_output(nn, deterministic=True)\n",
    "\n",
    "t_cost = lasagne.objectives.categorical_crossentropy(t_output, t_target).mean()\n",
    "t_acc = lasagne.objectives.categorical_accuracy(t_output, t_target).mean()\n",
    "\n",
    "t_cost_det = lasagne.objectives.categorical_crossentropy(t_output_det, t_target).mean()\n",
    "t_acc_det = lasagne.objectives.categorical_accuracy(t_output_det, t_target).mean()\n",
    "\n",
    "params = LL.get_all_params(nn)\n",
    "\n",
    "l2 = lasagne.regularization.regularize_network_params(nn, lasagne.regularization.l2)\n",
    "updates = lasagne.updates.adam(t_cost + L2*l2, params)\n",
    "\n",
    "train_fn = theano.function([l_in.input_var, t_target], t_cost, updates=updates)\n",
    "cost_fn = theano.function([l_in.input_var, t_target], [t_cost_det, t_acc_det])\n",
    "\n",
    "forward_fn = theano.function([l_in.input_var], t_output_det)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "id_generator = cycle_rand(idx_train)\n",
    "\n",
    "test_x, test_y = generate_data(idx_test, lucene_roman)\n",
    "\n",
    "BATCH = 100\n",
    "\n",
    "rows_seen = 0\n",
    "while rows_seen < 300000:\n",
    "    indices = take(BATCH, id_generator)\n",
    "    batch_x, batch_y = shuffle_dataset(*generate_data(indices, lucene_roman))\n",
    "    costs.append(train_fn(batch_x, batch_y))\n",
    "    rows_seen += BATCH\n",
    "    if rows_seen % 10000 < BATCH:\n",
    "        nll, acc = cost_fn(test_x, test_y)\n",
    "        mean_acc = (test_x.reshape((test_x.shape[0], 4, 50)).sum(axis=2).argmax(axis=1) == test_y).sum() / test_y.shape[0]\n",
    "        print(rows_seen, np.mean(costs), nll, acc, mean_acc)\n",
    "        costs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lucene_train_pred = forward_fn(generate_data(idx_train, lucene_roman)[0])\n",
    "lucene_test_pred = forward_fn(generate_data(idx_test, lucene_roman)[0])\n",
    "lucene_valid_pred = forward_fn(generate_data(idx_valid, lucene_roman)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Averaging\n",
    "-------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_, train_y = generate_data(idx_train, ngrams)\n",
    "\n",
    "for k_lucene in np.linspace(1, 2, 5):\n",
    "    for k_reptil in np.linspace(1, 2, 5):\n",
    "\n",
    "        average_train_pred = lucene_train_pred*k_lucene + reptil_train_pred*k_reptil + ngrams_train_pred\n",
    "        average_test_pred = lucene_test_pred*k_lucene + reptil_test_pred*k_reptil + ngrams_test_pred\n",
    "\n",
    "        c = (k_lucene, k_reptil)\n",
    "#         print('Training average', c, (average_train_pred.argmax(axis=1) == train_y).sum() / train_y.shape)\n",
    "        print('Testing average', c, (average_test_pred.argmax(axis=1) == test_y).sum() / test_y.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking\n",
    "----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.hstack([lucene_train_pred, reptil_train_pred, ngrams_train_pred])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
