{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, division, unicode_literals\n",
    "import os, sys\n",
    "from os.path import join\n",
    "import json\n",
    "from codecs import open\n",
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import random\n",
    "from itertools import chain\n",
    "from time import time\n",
    "from nltk.stem import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ROOT_DATA = join(os.environ[\"HOME\"], \"data/allen-ai-challenge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Datasets\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "WIKI = join(ROOT_DATA, \"parsed_wiki_data\") #top5 search wiki hits from wiki\n",
    "CK12 = join(ROOT_DATA, \"ck12_dump\") #parsing ck12\n",
    "QUIZLET = join(ROOT_DATA, 'quizlet')\n",
    "TOPKEK = join(ROOT_DATA, 'studystack')\n",
    "\n",
    "TRAINING = join(ROOT_DATA, \"training_set.tsv\")\n",
    "TRAINING_CLEANED = join(ROOT_DATA, \"training_set_cleaned.tsv\")\n",
    "VALIDATION = join(ROOT_DATA, \"validation_set.tsv\")\n",
    "VALIDATION_CLEANED = join(ROOT_DATA, \"validation_set_cleaned.tsv\")\n",
    "\n",
    "CORPUS_PARAGRAPH = join(ROOT_DATA, \"corpus_paragraph.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    return [stemmer.stem(w) for w in nltk.word_tokenize(text.lower()) if w not in stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 119 ms, sys: 14.3 ms, total: 133 ms\n",
      "Wall time: 133 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ck12_paragraphs = []\n",
    "for i, fn_short in enumerate(os.listdir(CK12)):\n",
    "    fn = join(CK12, fn_short)\n",
    "    with open(fn, encoding='utf-8', errors='ignore') as f:\n",
    "        ck12_article = json.load(f)\n",
    "        for subtitle, paragraph in ck12_article['contents'].items():\n",
    "            ck12_paragraphs.append(paragraph.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7148"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ck12_paragraphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.58 s, sys: 219 ms, total: 5.8 s\n",
      "Wall time: 5.81 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "wiki_paragraphs = []\n",
    "for i, fn_short in enumerate(os.listdir(WIKI)):\n",
    "    fn = join(WIKI, fn_short)\n",
    "    with open(fn, encoding='utf-8', errors='ignore') as f:\n",
    "        for line in (line.strip() for line in f):\n",
    "            if not line:\n",
    "                continue\n",
    "            if line.startswith('=='):\n",
    "                continue\n",
    "            wiki_paragraphs.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "561764"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wiki_paragraphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "geology.txt: 211689\n",
      "biology.txt: 238449\n",
      "chem.txt: 186910\n",
      "ck-12.txt: 9811\n",
      "ck 12.txt: 9811\n",
      "space.txt: 103268\n",
      "lifescience.txt: 7104\n",
      "earthscience.txt: 8281\n",
      "climate.txt: 107490\n",
      "anatomy.txt: 275689\n",
      "physicalscience.txt: 4753\n",
      "physiology.txt: 288193\n",
      "earth science.txt: 180614\n",
      "botany.txt: 247006\n",
      "astronomy.txt: 190219\n",
      "science.txt: 164241\n",
      "life science.txt: 166535\n",
      "physical science.txt: 191991\n",
      "chemistry.txt: 209191\n",
      "physics.txt: 196988\n",
      "genetics.txt: 181100\n",
      "ck12.txt: 4632\n",
      "DNA.txt: 126230\n",
      "\n",
      "CPU times: user 27.4 s, sys: 730 ms, total: 28.2 s\n",
      "Wall time: 28.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from StringIO import StringIO\n",
    "terms = {}\n",
    "for i, fn_short in enumerate(os.listdir(QUIZLET)):\n",
    "    term_count = 0\n",
    "    fn = join(QUIZLET, fn_short)\n",
    "    with open(fn, encoding='utf-8', errors='ignore') as f:\n",
    "        for line in f:\n",
    "            j = json.load(StringIO(line))\n",
    "            for t in j['terms']:\n",
    "                terms[t['term']] = t['definition']\n",
    "                term_count += 1\n",
    "    print(fn_short, term_count, sep=': ')\n",
    "print()\n",
    "quizlet_paragraphs = [t + ' ' + d for t, d in terms.iteritems()]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1194285"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(quizlet_paragraphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ch_docs.txt: 426147\n",
      "gen_docs.txt: 17119\n",
      "es_docs.txt: 459625\n",
      "bio_docs.txt: 1736698\n",
      "apgeo_docs.txt: 81173\n",
      "ph_docs.txt: 106345\n",
      "science_docs.txt: 1094720\n",
      "geo_docs.txt: 332818\n",
      "anth_docs.txt: 36011\n",
      "ps_docs.txt: 413622\n",
      "\n",
      "CPU times: user 5.54 s, sys: 1.59 s, total: 7.13 s\n",
      "Wall time: 7.12 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "topkek_cards = []\n",
    "for i, path in enumerate(os.listdir(TOPKEK)):\n",
    "    fn = join(TOPKEK, path)\n",
    "    with open(fn, encoding='utf-8', errors='ignore') as f:\n",
    "        docs = f.readlines()\n",
    "        for d in docs:\n",
    "            topkek_cards.append(d.strip())\n",
    "        print(path, len(docs), sep=': ')\n",
    "topkek_cards = set(topkek_cards)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1927951"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(topkek_cards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning datasets\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords = set(stopwords.words('english') + '. , ! ? !? ?! ... ; : - â€” summary youtube www'.split())\n",
    "for t in \"no not above\".split():\n",
    "    stopwords.remove(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def text_clean(text):\n",
    "    s = re.sub(r'[^\\w\\s\\d]', '', text)\n",
    "    return [stemmer.stem(w) for w in nltk.word_tokenize(s.lower()) if w not in stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'hello', u'die', u'went', u'abov', u'45', u'5']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_clean(\"hello Died \\\\ went I Summary all of the above 45 5 www youtube\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "c = 0\n",
    "with open(CORPUS_PARAGRAPH, encoding=\"utf-8\", mode=\"w\") as f:\n",
    "    for d in chain(ck12_paragraphs, wiki_paragraphs, quizlet_paragraphs, topkek_cards):\n",
    "        ct = text_clean(d)\n",
    "        print(*ct, sep=\" \", file=f)\n",
    "        c += 1\n",
    "print('Wrote', c, 'paragraphs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train/validation\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def text_clean_join(t):\n",
    "    return \" \".join(text_clean(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(TRAINING_CLEANED, encoding=\"utf-8\", mode=\"w\") as fo:\n",
    "    with open(TRAINING, encoding=\"utf-8\") as f:\n",
    "        next(f)\n",
    "        for i, l in enumerate(f):\n",
    "            [qid, q, r, aa, ab, ac, ad] = l.strip().split(\"\\t\")\n",
    "            print(qid, text_clean_join(q), r,\n",
    "                  text_clean_join(aa),\n",
    "                  text_clean_join(ab),\n",
    "                  text_clean_join(ac),\n",
    "                  text_clean_join(ad),\n",
    "                  sep=\"\\t\", file=fo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(VALIDATION_CLEANED, encoding=\"utf-8\", mode=\"w\") as fo:\n",
    "    with open(VALIDATION, encoding=\"utf-8\") as f:\n",
    "        next(f)\n",
    "        for i, l in enumerate(f):\n",
    "            [qid, q, aa, ab, ac, ad] = l.strip().split(\"\\t\")\n",
    "            print(qid, text_clean_join(q),\n",
    "                  text_clean_join(aa),\n",
    "                  text_clean_join(ab),\n",
    "                  text_clean_join(ac),\n",
    "                  text_clean_join(ad),\n",
    "                  sep=\"\\t\", file=fo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'+'.join('104080  germin compos mollusk kingdom   scallop protista        dna molecul     '.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
