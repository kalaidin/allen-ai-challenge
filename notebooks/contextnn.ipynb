{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, division, unicode_literals\n",
    "import six\n",
    "import os, sys\n",
    "from os.path import join\n",
    "import json\n",
    "from codecs import open\n",
    "from collections import defaultdict\n",
    "from operator import itemgetter\n",
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import codecs\n",
    "import random\n",
    "from time import time\n",
    "\n",
    "from nltk.stem import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "from nltk.corpus import stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<jcc.JCCEnv at 0x7f17249ebbb8>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import lucene\n",
    "from org.apache.lucene.analysis.standard import StandardAnalyzer\n",
    "from org.apache.lucene.document import Document, Field\n",
    "from org.apache.lucene.index import IndexWriter, IndexWriterConfig, IndexReader\n",
    "from org.apache.lucene.search import IndexSearcher\n",
    "from org.apache.lucene.search import Sort, SortField\n",
    "from org.apache.lucene.queryparser.classic import QueryParser\n",
    "from org.apache.lucene.store import SimpleFSDirectory\n",
    "from org.apache.lucene.util import Version\n",
    "from java.io import File\n",
    "lucene.initVM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATA_DIR = join(os.environ['HOME'], 'data/allen-ai-challenge')\n",
    "WIKI_DIR = join(DATA_DIR, 'wiki_dump')\n",
    "CK12_DIR = join(DATA_DIR, 'ck12_dump')\n",
    "TRAINING_SET = join(DATA_DIR, 'training_set.tsv')\n",
    "VALIDATION_SET = join(DATA_DIR, 'validation_set.tsv')\n",
    "TRAINING_SET_MERGED = join(DATA_DIR, 'training_set_merged.tsv')\n",
    "# INDEX_DIR = join(DATA_DIR, 'index-wiki-ck12')\n",
    "# INDEX_DIR = join(DATA_DIR, 'index-ck12-stem')\n",
    "# INDEX_DIR = join(DATA_DIR, 'index-all-l_stem_summ')\n",
    "INDEX_DIR = join(DATA_DIR, 'index-ck12-stem')\n",
    "SUBMISSION = join(DATA_DIR, 'submissions/lucene_wiki_ck12_17jan.tsv')\n",
    "VOCABULARY = join(DATA_DIR, 'vocabulary', 'w2v_a2_5.tsv')\n",
    "SENT_DELIM = ' | '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords = set(stopwords.words('english') + '. , ! ? !? ?! ... ; : - â€”'.split())\n",
    "def cleanup_text(text):\n",
    "    sents = []\n",
    "    for s in nltk.sent_tokenize(text):        \n",
    "#         words = [stemmer.stem(w) for w in nltk.word_tokenize(s.lower()) if w not in stopwords]\n",
    "        words = [w for w in nltk.word_tokenize(s.lower()) if w not in stopwords]\n",
    "        if words:\n",
    "            sents.append(words)\n",
    "    return SENT_DELIM.join([' '.join(s) for s in sents])\n",
    "#     return [stemmer.stem(w.text) for w in nlp(text) if not w.is_stop and not w.is_punct and not w.is_space]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.15 s, sys: 16 ms, total: 1.17 s\n",
      "Wall time: 1.17 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "vocab = {}\n",
    "with open(VOCABULARY, encoding='utf8') as f:\n",
    "    for word, vec in (line.strip().split('\\t', 1) for line in f):\n",
    "        vocab[word] = np.fromstring(vec, sep='\\t')\n",
    "vocab_dim = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocab['earth'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cleanup_text('Here is some text. What the heck more place?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Index Creation\n",
    "----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "analyzer = StandardAnalyzer(Version.LUCENE_4_10_1)\n",
    "writerConfig = IndexWriterConfig(Version.LUCENE_4_10_1, StandardAnalyzer())\n",
    "writer = IndexWriter(SimpleFSDirectory(File(INDEX_DIR)), writerConfig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_document(doc_text):\n",
    "    doc = Document()\n",
    "    doc.add(Field(\"text\", cleanup_text(doc_text), Field.Store.YES, Field.Index.ANALYZED))\n",
    "    writer.addDocument(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 18.8 s, sys: 68 ms, total: 18.9 s\n",
      "Wall time: 17.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for i, fn_short in enumerate(os.listdir(CK12_DIR)):\n",
    "    fn = join(CK12_DIR, fn_short)\n",
    "    with open(fn, encoding='utf-8', errors='ignore') as f:\n",
    "        ck12_article = json.load(f)\n",
    "#         content = []\n",
    "#         for subtitle, paragraph in ck12_article['contents'].items():\n",
    "#             content.append(subtitle + '. ' + paragraph)\n",
    "#         add_document(' '.join(content))    \n",
    "\n",
    "        for subtitle, paragraph in ck12_article['contents'].items():\n",
    "            add_document(subtitle + '. ' + paragraph)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7148"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_count = writer.numDocs()\n",
    "writer.close()\n",
    "doc_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build NN\n",
    "----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import theano\n",
    "import theano.tensor as T\n",
    "import lasagne\n",
    "import lasagne.layers as LL\n",
    "from lasagne.nonlinearities import elu, rectify\n",
    "\n",
    "M = 2.0\n",
    "\n",
    "input_context = LL.InputLayer((None, 300))\n",
    "input_hyp = LL.InputLayer((None, 300))\n",
    "\n",
    "# l_context = LL.ReshapeLayer(input_context, ([0], [1]))\n",
    "# l_hyp = LL.ReshapeLayer(input_hyp, ([0], [1]))\n",
    "\n",
    "l_diff = LL.ElemwiseMergeLayer([input_context, input_hyp], merge_function=T.sub)\n",
    "l_mult = LL.ElemwiseMergeLayer([input_context, input_hyp], merge_function=T.mul)\n",
    "nn = LL.concat([l_diff, l_mult])\n",
    "nn = LL.DenseLayer(nn, 100, nonlinearity=elu)\n",
    "nn = LL.DenseLayer(nn, 1, nonlinearity=rectify)\n",
    "t_output = LL.get_output(nn)[:, 0]\n",
    "\n",
    "t_must_be_less = t_output[0::2]\n",
    "t_must_be_more = t_output[1::2]\n",
    "\n",
    "t_cost = (T.sqr(t_must_be_less) + T.sqr(T.maximum(0, M - t_must_be_more))).mean()\n",
    "\n",
    "params = LL.get_all_params(nn)\n",
    "\n",
    "updates = lasagne.updates.adam(t_cost, params)\n",
    "\n",
    "train_fn = theano.function([input_hyp.input_var, input_context.input_var], t_cost, updates=updates)\n",
    "cost_fn = theano.function([input_hyp.input_var, input_context.input_var], t_cost)\n",
    "energy_fn = theano.function([input_hyp.input_var, input_context.input_var], t_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read index\n",
    "-----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "analyzer = StandardAnalyzer(Version.LUCENE_4_10_1)\n",
    "reader = IndexReader.open(SimpleFSDirectory(File(INDEX_DIR)))\n",
    "searcher = IndexSearcher(reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "indexi = np.arange(reader.maxDoc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def drop_random(seq):\n",
    "    '''Changes seq!'''\n",
    "    idx = random.randint(0, len(seq)-1)\n",
    "    d = seq[idx]\n",
    "    del seq[idx]\n",
    "    return d, seq    \n",
    "\n",
    "def corrupt_context(index, window_nearest, window_farest):    \n",
    "    artice = reader.document(index)['text']\n",
    "    sentsA = artice.split(SENT_DELIM)\n",
    "    if len(sentsA) < 2:\n",
    "        return None, None\n",
    "    hypA, restA = drop_random(sentsA)\n",
    "    others = [i for i in range(index - window_farest, index + window_farest + 1) \n",
    "              if (np.abs(index-i) > window_nearest) and \n",
    "                 (i >= 0) and (i < doc_count)]\n",
    "    idxB = random.choice(others)\n",
    "    sentsB = reader.document(idxB)['text'].split(SENT_DELIM)\n",
    "    return [(hypA, ' '.join(restA)), (hypA, ' '.join(sentsB))]\n",
    "\n",
    "def mean_w2v(text):\n",
    "    vec = np.zeros((vocab_dim,), dtype='float64')\n",
    "    c = 1\n",
    "    for w in nltk.word_tokenize(text):\n",
    "        if w in vocab:\n",
    "            vec += vocab[w]\n",
    "            c += 1\n",
    "    return (vec/c).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 16.1 s, sys: 256 ms, total: 16.4 s\n",
      "Wall time: 13.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "questions = []\n",
    "with open(TRAINING_SET, encoding='utf8') as f:\n",
    "    f.readline()\n",
    "    for line in f:\n",
    "        qid, q, correct, aa, ab, ac, ad = line.strip().split('\\t')\n",
    "\n",
    "        query = QueryParser(Version.LUCENE_4_10_1, \"text\", analyzer).parse(re.sub(\"[^a-zA-Z0-9]\",\" \", cleanup_text(q)))\n",
    "        hits = searcher.search(query, 20).scoreDocs\n",
    "        doc = reader.document(hits[0].doc)['text']\n",
    "        q_vec = mean_w2v(doc)\n",
    "        vecs_context = np.zeros((4, vocab_dim), dtype='float32')\n",
    "        vecs_context += q_vec\n",
    "        vecs_hyp = np.zeros((4, vocab_dim), dtype='float32')\n",
    "        for i, a in enumerate([aa, ab, ac, ad]):\n",
    "            try:\n",
    "                query = QueryParser(Version.LUCENE_4_10_1, \"text\", analyzer).parse(re.sub(\"[^a-zA-Z0-9]\",\" \", cleanup_text(a)))\n",
    "                hits = searcher.search(query, 20).scoreDocs\n",
    "                doc = reader.document(hits[0].doc)['text']\n",
    "            except:\n",
    "                doc = ''\n",
    "            vecs_hyp[i] = mean_w2v(doc)\n",
    "            questions.append((qid, 'ABCD'.index(correct), vecs_hyp, vecs_context))\n",
    "\n",
    "\n",
    "def check():\n",
    "    scores = []\n",
    "    for qid, idx_correct, vecs_hyp, vecs_context in questions:\n",
    "        energies = energy_fn(vecs_hyp, vecs_context)\n",
    "        scores.append(np.argmin(energies) == idx_correct)\n",
    "    return np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 1.230 (33.64%) in 20s\n",
      "1: 0.768 (33.44%) in 20s\n",
      "2: 0.776 (33.04%) in 20s\n",
      "3: 0.797 (34.44%) in 20s\n",
      "4: 0.914 (33.28%) in 20s\n",
      "5: 1.115 (34.08%) in 20s\n",
      "6: 1.367 (34.84%) in 20s\n",
      "7: 1.307 (33.68%) in 20s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-5bbafa029e74>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mu'time'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34mu''\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34mu\"\\nBATCH = 20\\nEPOCHS = 100\\n    \\nmin_win = [10, 5, 4, 3, 2, 1] + [0] * EPOCHS    \\n\\nfor e in range(EPOCHS):\\n    time_started = time()\\n    indices = np.arange(doc_count, dtype=int)\\n    np.random.shuffle(indices)\\n    costs = []\\n    for i in xrange(0, indices.shape[0], BATCH):\\n        batch_idx = indices[i:i+BATCH]\\n        batch_hyp = np.zeros((BATCH*2, vocab_dim), dtype='float32')\\n        batch_context = np.zeros((BATCH*2, vocab_dim), dtype='float32')\\n        for b, idx_b in enumerate(batch_idx):\\n            right, corrupted = corrupt_context(int(idx_b), min_win[e], min_win[e] + 5)\\n            batch_hyp[b*2] = mean_w2v(right[0])\\n            batch_context[b*2] = mean_w2v(right[1])\\n\\n            batch_hyp[b*2+1] = mean_w2v(corrupted[0])\\n            batch_context[b*2+1] = mean_w2v(corrupted[1])\\n        batch_cost = train_fn(batch_hyp[:b*2], batch_context[:b*2])\\n        costs.append(batch_cost)\\n#         print(batch_energy)\\n    print('%d: %.3f (%.2f%%) in %.0fs' % (e, np.mean(costs), check() * 100, time() - time_started))\\n    sys.stdout.flush()\\n    \"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.pyc\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[1;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[0;32m   2291\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2292\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2293\u001b[1;33m                 \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2294\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2295\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<decorator-gen-60>\u001b[0m in \u001b[0;36mtime\u001b[1;34m(self, line, cell, local_ns)\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/IPython/core/magic.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(f, *a, **k)\u001b[0m\n\u001b[0;32m    191\u001b[0m     \u001b[1;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 193\u001b[1;33m         \u001b[0mcall\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    194\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/IPython/core/magics/execution.pyc\u001b[0m in \u001b[0;36mtime\u001b[1;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[0;32m   1165\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1166\u001b[0m             \u001b[0mst\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1167\u001b[1;33m             \u001b[1;32mexec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1168\u001b[0m             \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1169\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36mcheck\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    857\u001b[0m         \u001b[0mt0_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    858\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 859\u001b[1;33m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    860\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    861\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'position_of_error'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "BATCH = 20\n",
    "EPOCHS = 100\n",
    "    \n",
    "min_win = [10, 5, 4, 3, 2, 1] + [0] * EPOCHS    \n",
    "\n",
    "for e in range(EPOCHS):\n",
    "    time_started = time()\n",
    "    indices = np.arange(doc_count, dtype=int)\n",
    "    np.random.shuffle(indices)\n",
    "    costs = []\n",
    "    for i in xrange(0, indices.shape[0], BATCH):\n",
    "        batch_idx = indices[i:i+BATCH]\n",
    "        batch_hyp = np.zeros((BATCH*2, vocab_dim), dtype='float32')\n",
    "        batch_context = np.zeros((BATCH*2, vocab_dim), dtype='float32')\n",
    "        for b, idx_b in enumerate(batch_idx):\n",
    "            right, corrupted = corrupt_context(int(idx_b), min_win[e], min_win[e] + 5)\n",
    "            batch_hyp[b*2] = mean_w2v(right[0])\n",
    "            batch_context[b*2] = mean_w2v(right[1])\n",
    "\n",
    "            batch_hyp[b*2+1] = mean_w2v(corrupted[0])\n",
    "            batch_context[b*2+1] = mean_w2v(corrupted[1])\n",
    "        batch_cost = train_fn(batch_hyp[:b*2], batch_context[:b*2])\n",
    "        costs.append(batch_cost)\n",
    "#         print(batch_energy)\n",
    "    print('%d: %.3f (%.2f%%) in %.0fs' % (e, np.mean(costs), check() * 100, time() - time_started))\n",
    "    sys.stdout.flush()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reader.document(553)['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def check():\n",
    "    with open(TRAINING_SET, encoding='utf8') as f:\n",
    "        f.readline()\n",
    "        for line in f:\n",
    "            qid, q, correct, aa, ab, ac, ad = line.strip().split('\\t')\n",
    "            \n",
    "            print(cleanup_text(q))\n",
    "            break\n",
    "check()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
