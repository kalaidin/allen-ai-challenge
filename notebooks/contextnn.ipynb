{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, division, unicode_literals\n",
    "import six\n",
    "import os, sys\n",
    "from os.path import join\n",
    "import json\n",
    "from codecs import open\n",
    "from collections import defaultdict\n",
    "from operator import itemgetter\n",
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import codecs\n",
    "import random\n",
    "from time import time\n",
    "\n",
    "from nltk.stem import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<jcc.JCCEnv at 0x7f59c031f900>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import lucene\n",
    "from org.apache.lucene.analysis.standard import StandardAnalyzer\n",
    "from org.apache.lucene.document import Document, Field\n",
    "from org.apache.lucene.index import IndexWriter, IndexWriterConfig, IndexReader\n",
    "from org.apache.lucene.search import IndexSearcher\n",
    "from org.apache.lucene.search import Sort, SortField\n",
    "from org.apache.lucene.queryparser.classic import QueryParser\n",
    "from org.apache.lucene.store import SimpleFSDirectory\n",
    "from org.apache.lucene.util import Version\n",
    "from java.io import File\n",
    "lucene.initVM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATA_DIR = join(os.environ['HOME'], 'data/allen-ai-challenge')\n",
    "WIKI_DIR = join(DATA_DIR, 'wiki_dump')\n",
    "CK12_DIR = join(DATA_DIR, 'ck12_dump')\n",
    "TRAINING_SET = join(DATA_DIR, 'training_set.tsv')\n",
    "VALIDATION_SET = join(DATA_DIR, 'validation_set.tsv')\n",
    "TRAINING_SET_MERGED = join(DATA_DIR, 'training_set_merged.tsv')\n",
    "# INDEX_DIR = join(DATA_DIR, 'index-wiki-ck12')\n",
    "# INDEX_DIR = join(DATA_DIR, 'index-ck12-stem')\n",
    "# INDEX_DIR = join(DATA_DIR, 'index-all-l_stem_summ')\n",
    "INDEX_DIR = join(DATA_DIR, 'index-ck12-stem')\n",
    "SUBMISSION = join(DATA_DIR, 'submissions/lucene_wiki_ck12_17jan.tsv')\n",
    "VOCABULARY = join(DATA_DIR, 'vocabulary', 'w2v_a2_5.tsv')\n",
    "SENT_DELIM = ' | '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords = set(stopwords.words('english') + '. , ! ? !? ?! ... ; : - â€”'.split())\n",
    "def cleanup_text(text):\n",
    "    sents = []\n",
    "    for s in nltk.sent_tokenize(text):        \n",
    "#         words = [stemmer.stem(w) for w in nltk.word_tokenize(s.lower()) if w not in stopwords]\n",
    "        words = [w for w in nltk.word_tokenize(s.lower()) if w not in stopwords]\n",
    "        if words:\n",
    "            sents.append(words)\n",
    "    return SENT_DELIM.join([' '.join(s) for s in sents])\n",
    "#     return [stemmer.stem(w.text) for w in nlp(text) if not w.is_stop and not w.is_punct and not w.is_space]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.47 s, sys: 20 ms, total: 1.49 s\n",
      "Wall time: 1.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "vocab = {}\n",
    "with open(VOCABULARY, encoding='utf8') as f:\n",
    "    for word, vec in (line.strip().split('\\t', 1) for line in f):\n",
    "        vocab[word] = np.fromstring(vec, sep='\\t')\n",
    "vocab_dim = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab['earth'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'text | heck place'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleanup_text('Here is some text. What the heck more place?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Index Creation\n",
    "----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "analyzer = StandardAnalyzer(Version.LUCENE_4_10_1)\n",
    "writerConfig = IndexWriterConfig(Version.LUCENE_4_10_1, StandardAnalyzer())\n",
    "writer = IndexWriter(SimpleFSDirectory(File(INDEX_DIR)), writerConfig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_document(doc_text):\n",
    "    doc = Document()\n",
    "    doc.add(Field(\"text\", cleanup_text(doc_text), Field.Store.YES, Field.Index.ANALYZED))\n",
    "    writer.addDocument(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 25.7 s, sys: 232 ms, total: 26 s\n",
      "Wall time: 33.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# CK12\n",
    "for i, fn_short in enumerate(os.listdir(CK12_DIR)):\n",
    "    fn = join(CK12_DIR, fn_short)\n",
    "    with open(fn, encoding='utf-8', errors='ignore') as f:\n",
    "        ck12_article = json.load(f)\n",
    "#         content = []\n",
    "#         for subtitle, paragraph in ck12_article['contents'].items():\n",
    "#             content.append(subtitle + '. ' + paragraph)\n",
    "#         add_document(' '.join(content))    \n",
    "\n",
    "        for subtitle, paragraph in ck12_article['contents'].items():\n",
    "            add_document(subtitle + '. ' + paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5min 17s, sys: 1.91 s, total: 5min 19s\n",
      "Wall time: 7min 19s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Wiki\n",
    "for i, fn_short in enumerate(os.listdir(WIKI_DIR)):\n",
    "    fn = join(WIKI_DIR, fn_short)\n",
    "    with open(fn, encoding='utf-8', errors='ignore') as f:\n",
    "        wiki_article = json.load(f)\n",
    "        _, summary, content = wiki_article\n",
    "        add_document(summary)\n",
    "        for p in re.sub('\\[ \\d* \\]', ' ', content).replace('\\n\\n\\n', '.\\n ').split('\\n\\n'):\n",
    "            add_document(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "243469"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_count = writer.numDocs()\n",
    "writer.close()\n",
    "doc_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build NN\n",
    "----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.77 s, sys: 16 ms, total: 1.79 s\n",
      "Wall time: 1.78 s\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "# VERSION 0\n",
    "# import theano\n",
    "# import theano.tensor as T\n",
    "# import lasagne\n",
    "# import lasagne.layers as LL\n",
    "# from lasagne.nonlinearities import elu, rectify\n",
    "\n",
    "# M = 1.0\n",
    "\n",
    "# input_context = LL.InputLayer((None, 300))\n",
    "# input_hyp = LL.InputLayer((None, 300))\n",
    "\n",
    "# l_diff = LL.ElemwiseMergeLayer([input_context, input_hyp], merge_function=T.sub)\n",
    "# l_mult = LL.ElemwiseMergeLayer([input_context, input_hyp], merge_function=T.mul)\n",
    "\n",
    "# nn = LL.concat([l_diff, l_mult])\n",
    "# nn = LL.DenseLayer(nn, 100, nonlinearity=elu)\n",
    "# nn = LL.DenseLayer(nn, 1, nonlinearity=rectify)\n",
    "# t_output = LL.get_output(nn)[:, 0]\n",
    "\n",
    "# t_must_be_less = t_output[0::2]\n",
    "# t_must_be_more = t_output[1::2]\n",
    "\n",
    "# # t_cost = (T.maximum(0, M + t_must_be_less - t_must_be_more)).mean()\n",
    "# t_cost = (T.maximum(0, M + T.sqr(t_must_be_less) - T.sqr(t_must_be_more))).mean()\n",
    "# # t_cost = (T.sqr(t_must_be_less) + T.sqr(T.maximum(0, M - t_must_be_more))).mean()  # square-square\n",
    "# # t_cost = (T.sqr(t_must_be_less) + T.exp(-t_must_be_more)).mean()  # square-exponential\n",
    "\n",
    "# params = LL.get_all_params(nn)\n",
    "\n",
    "# updates = lasagne.updates.adam(t_cost, params)\n",
    "\n",
    "# train_fn = theano.function([input_hyp.input_var, input_context.input_var], t_cost, updates=updates)\n",
    "# cost_fn = theano.function([input_hyp.input_var, input_context.input_var], t_cost)\n",
    "# energy_fn = theano.function([input_hyp.input_var, input_context.input_var], t_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 34.9 s, sys: 1.35 s, total: 36.3 s\n",
      "Wall time: 1min 42s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# VERSION 1\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import lasagne\n",
    "import lasagne.layers as LL\n",
    "from lasagne.nonlinearities import elu, rectify\n",
    "\n",
    "M = 1.0\n",
    "\n",
    "LEN_CONTEXT = 50\n",
    "LEN_HYP = 20\n",
    "\n",
    "input_context = LL.InputLayer((None, LEN_CONTEXT, 300))\n",
    "input_context_mask = LL.InputLayer((None, LEN_CONTEXT))\n",
    "l_c = LL.LSTMLayer(input_context, 300, only_return_final=True, mask_input=input_context_mask)\n",
    "\n",
    "input_hyp = LL.InputLayer((None, LEN_HYP, 300))\n",
    "input_hyp_mask = LL.InputLayer((None, LEN_HYP))\n",
    "l_h = LL.LSTMLayer(input_hyp, 300, only_return_final=True, mask_input=input_hyp_mask)\n",
    "\n",
    "nn = LL.concat([l_c, l_h])\n",
    "nn = LL.DenseLayer(nn, 100, nonlinearity=elu)\n",
    "nn = LL.DenseLayer(nn, 1, nonlinearity=rectify)\n",
    "t_output = LL.get_output(nn)[:, 0]\n",
    "\n",
    "t_must_be_less = t_output[0::2]\n",
    "t_must_be_more = t_output[1::2]\n",
    "\n",
    "# t_cost = (T.maximum(0, M + t_must_be_less - t_must_be_more)).mean()\n",
    "t_cost = (T.maximum(0, M + T.sqr(t_must_be_less) - T.sqr(t_must_be_more))).mean()\n",
    "# t_cost = (T.sqr(t_must_be_less) + T.sqr(T.maximum(0, M - t_must_be_more))).mean()  # square-square\n",
    "# t_cost = (T.sqr(t_must_be_less) + T.exp(-t_must_be_more)).mean()  # square-exponential\n",
    "\n",
    "params = LL.get_all_params(nn)\n",
    "\n",
    "updates = lasagne.updates.adam(t_cost, params)\n",
    "\n",
    "train_fn = theano.function([input_hyp.input_var, input_hyp_mask.input_var,\n",
    "                            input_context.input_var, input_context_mask.input_var], t_cost, updates=updates)\n",
    "cost_fn = theano.function([input_hyp.input_var, input_hyp_mask.input_var,\n",
    "                            input_context.input_var, input_context_mask.input_var], t_cost)\n",
    "energy_fn = theano.function([input_hyp.input_var, input_hyp_mask.input_var,\n",
    "                            input_context.input_var, input_context_mask.input_var], t_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# VERSION 1\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import lasagne\n",
    "import lasagne.layers as LL\n",
    "from lasagne.nonlinearities import elu, rectify\n",
    "\n",
    "M = 1.0\n",
    "\n",
    "LEN_CONTEXT = 50\n",
    "LEN_HYP = 20\n",
    "\n",
    "input_context = LL.InputLayer((None, LEN_CONTEXT, 300))\n",
    "input_context_mask = LL.InputLayer((None, LEN_CONTEXT))\n",
    "l_c = LL.LSTMLayer(input_context, 300, only_return_final=True, mask_input=input_context_mask)\n",
    "\n",
    "input_hyp = LL.InputLayer((None, LEN_HYP, 300))\n",
    "input_hyp_mask = LL.InputLayer((None, LEN_HYP))\n",
    "l_h = LL.LSTMLayer(input_hyp, 300, only_return_final=True, mask_input=input_hyp_mask)\n",
    "\n",
    "nn = LL.concat([l_c, l_h])\n",
    "nn = LL.DenseLayer(nn, 100, nonlinearity=elu)\n",
    "nn = LL.DenseLayer(nn, 1, nonlinearity=rectify)\n",
    "t_output = LL.get_output(nn)[:, 0]\n",
    "\n",
    "t_must_be_less = t_output[0::2]\n",
    "t_must_be_more = t_output[1::2]\n",
    "\n",
    "# t_cost = (T.maximum(0, M + t_must_be_less - t_must_be_more)).mean()\n",
    "t_cost = (T.maximum(0, M + T.sqr(t_must_be_less) - T.sqr(t_must_be_more))).mean()\n",
    "# t_cost = (T.sqr(t_must_be_less) + T.sqr(T.maximum(0, M - t_must_be_more))).mean()  # square-square\n",
    "# t_cost = (T.sqr(t_must_be_less) + T.exp(-t_must_be_more)).mean()  # square-exponential\n",
    "\n",
    "params = LL.get_all_params(nn)\n",
    "\n",
    "updates = lasagne.updates.adam(t_cost, params)\n",
    "\n",
    "train_fn = theano.function([input_hyp.input_var, input_hyp_mask.input_var,\n",
    "                            input_context.input_var, input_context_mask.input_var], t_cost, updates=updates)\n",
    "cost_fn = theano.function([input_hyp.input_var, input_hyp_mask.input_var,\n",
    "                            input_context.input_var, input_context_mask.input_var], t_cost)\n",
    "energy_fn = theano.function([input_hyp.input_var, input_hyp_mask.input_var,\n",
    "                            input_context.input_var, input_context_mask.input_var], t_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read index\n",
    "-----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "analyzer = StandardAnalyzer(Version.LUCENE_4_10_1)\n",
    "reader = IndexReader.open(SimpleFSDirectory(File(INDEX_DIR)))\n",
    "searcher = IndexSearcher(reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "indexi = np.arange(reader.maxDoc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def take_random(seq, drop=True):\n",
    "    '''Changes seq!'''\n",
    "    idx = random.randint(0, len(seq)-1)\n",
    "    d = seq[idx]\n",
    "    if drop:\n",
    "        del seq[idx]\n",
    "    return d, seq    \n",
    "\n",
    "def corrupt_context(index, window_nearest, window_farest, target='context', modify_context=True):    \n",
    "    artice = reader.document(index)['text']\n",
    "    sentsA = artice.split(SENT_DELIM)\n",
    "    if len(sentsA) < 2:\n",
    "        return None, None\n",
    "    hypA, restA = drop_random(sentsA, drop=modify_context)\n",
    "    others = [i for i in range(index - window_farest, index + window_farest + 1) \n",
    "              if (np.abs(index-i) > window_nearest) and \n",
    "                 (i >= 0) and (i < doc_count)]\n",
    "    idxB = random.choice(others)\n",
    "    sentsB = reader.document(idxB)['text'].split(SENT_DELIM)\n",
    "    if target == 'context':\n",
    "        return [(hypA, ' '.join(restA)), (hypA, ' '.join(sentsB))]\n",
    "    elif target == 'hyp':\n",
    "        hypB, restB = take_random(sentsB, drop=False)\n",
    "        return [(hypA, ' '.join(restA)), (hypB, ' '.join(restA))]\n",
    "    else:\n",
    "        raise RuntimeError('Unknown corruption target = %s' % context)\n",
    "\n",
    "\n",
    "def mean_w2v(text):\n",
    "    vec = np.zeros((vocab_dim,), dtype='float64')\n",
    "    c = 1\n",
    "    for w in nltk.word_tokenize(text):\n",
    "        if w in vocab:\n",
    "            vec += vocab[w]\n",
    "            c += 1\n",
    "    return (vec/c).astype('float32'), None\n",
    "\n",
    "\n",
    "def seq_w2v(text, max_len):\n",
    "    row = 0\n",
    "    words = list(nltk.word_tokenize(text))\n",
    "    n = max(max_len, len(words))\n",
    "    \n",
    "    vec = np.zeros((n, vocab_dim), dtype='float32')\n",
    "    mask = np.zeros((n,), dtype='int')\n",
    "\n",
    "    for w in words:\n",
    "        if w in vocab:\n",
    "            vec[row] = vocab[w]\n",
    "            mask[row] = 1\n",
    "            row += 1\n",
    "    return vec[-max_len:], mask[-max_len:]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 25.1 s, sys: 1.27 s, total: 26.4 s\n",
      "Wall time: 52.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "questions = []\n",
    "with open(TRAINING_SET, encoding='utf8') as f:\n",
    "    f.readline()\n",
    "    for line in f:\n",
    "        qid, q, correct, aa, ab, ac, ad = line.strip().split('\\t')\n",
    "\n",
    "        query = QueryParser(Version.LUCENE_4_10_1, \"text\", analyzer).parse(re.sub(\"[^a-zA-Z0-9]\",\" \", cleanup_text(q)))\n",
    "        hits = searcher.search(query, 20).scoreDocs\n",
    "        doc = reader.document(hits[0].doc)['text']\n",
    "        q_vec, q_mask = seq_w2v(doc, LEN_CONTEXT)\n",
    "        vecs_context = np.repeat(q_vec[np.newaxis], 4, axis=0)\n",
    "        vecs_context_mask = np.repeat(q_mask[np.newaxis], 4, axis=0)\n",
    "        vecs_hyp = np.zeros((4, LEN_HYP, vocab_dim), dtype='float32')\n",
    "        vecs_hyp_mask = np.zeros((4, LEN_HYP), dtype='int')\n",
    "        for i, a in enumerate([aa, ab, ac, ad]):\n",
    "            try:\n",
    "                query = QueryParser(Version.LUCENE_4_10_1, \"text\", analyzer).parse(re.sub(\"[^a-zA-Z0-9]\",\" \", cleanup_text(a)))\n",
    "                hits = searcher.search(query, 20).scoreDocs\n",
    "                doc = reader.document(hits[0].doc)['text']\n",
    "            except:\n",
    "                doc = ''\n",
    "            vecs_hyp[i], vecs_hyp_mask[i] = seq_w2v(doc, LEN_HYP)\n",
    "            questions.append((qid, 'ABCD'.index(correct), vecs_hyp, vecs_hyp_mask, vecs_context, vecs_context_mask))\n",
    "\n",
    "\n",
    "def check(questions):\n",
    "    scores = []\n",
    "    for qid, idx_correct, vecs_hyp, vecs_hyp_mask, vecs_context, vecs_context_mask in questions:\n",
    "        energies = energy_fn(vecs_hyp, vecs_hyp_mask, vecs_context, vecs_context_mask)\n",
    "        scores.append(np.argmin(energies) == idx_correct)\n",
    "    return np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def randomized_cyclic_generator(seq):\n",
    "    rseq = list(seq)\n",
    "    while True:\n",
    "        random.shuffle(rseq)\n",
    "        for s in rseq:\n",
    "            yield s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "drop_random() got an unexpected keyword argument 'drop'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-123-51ac5e28a19f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[0midx_row_in_db\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mid_generator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m         right, corrupted = corrupt_context(idx_row_in_db, min_win, win_min + win_width, \n\u001b[1;32m---> 30\u001b[1;33m                                            random.choice(corruption_targets), modify_context=drop_hyp_from_context)\n\u001b[0m\u001b[0;32m     31\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mright\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m             \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-118-b1a274866e28>\u001b[0m in \u001b[0;36mcorrupt_context\u001b[1;34m(index, window_nearest, window_farest, target, modify_context)\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentsA\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m     \u001b[0mhypA\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrestA\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdrop_random\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentsA\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodify_context\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m     others = [i for i in range(index - window_farest, index + window_farest + 1) \n\u001b[0;32m     16\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mwindow_nearest\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: drop_random() got an unexpected keyword argument 'drop'"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "\n",
    "BATCH = 20\n",
    "EPOCHS = 100\n",
    "            \n",
    "id_generator = iter(randomized_cyclic_generator(np.arange(doc_count, dtype=int)))\n",
    "\n",
    "corruption_targets = ['context'] + ['hyp']\n",
    "drop_hyp_from_context = False\n",
    "win_min = 5\n",
    "win_width = 5\n",
    "\n",
    "print_each_sec = 30\n",
    "\n",
    "time_started = time()\n",
    "# acc = check(questions)\n",
    "acc = 0\n",
    "costs = []\n",
    "rows_seen = 0\n",
    "while True:\n",
    "    batch_hyp = np.zeros((BATCH*2, LEN_HYP, vocab_dim), dtype='float32')\n",
    "    batch_hyp_m = np.zeros((BATCH*2, LEN_HYP), dtype=int)\n",
    "    batch_context = np.zeros((BATCH*2, LEN_CONTEXT, vocab_dim), dtype='float32')\n",
    "    batch_context_m = np.zeros((BATCH*2, LEN_CONTEXT), dtype=int)\n",
    "\n",
    "    b = 0\n",
    "    while b < BATCH:\n",
    "        idx_row_in_db = int(next(id_generator))\n",
    "        right, corrupted = corrupt_context(idx_row_in_db, min_win, win_min + win_width, \n",
    "                                           random.choice(corruption_targets), modify_context=drop_hyp_from_context)\n",
    "        if right is None:\n",
    "            continue\n",
    "\n",
    "        batch_hyp[b*2], batch_hyp_m[b*2] = seq_w2v(right[0], LEN_HYP)\n",
    "        batch_context[b*2], batch_context_m[b*2] = seq_w2v(right[1], LEN_CONTEXT)\n",
    "\n",
    "        batch_hyp[b*2+1], batch_hyp_m[b*2+1] = seq_w2v(corrupted[0], LEN_HYP)\n",
    "        batch_context[b*2+1], batch_context_m[b*2+1] = seq_w2v(corrupted[1], LEN_CONTEXT)\n",
    "        b += 1\n",
    "            \n",
    "    batch_cost = train_fn(batch_hyp, batch_hyp_m, batch_context, batch_context_m)\n",
    "#     print(batch_cost)\n",
    "    costs.append(batch_cost)\n",
    "    rows_seen += BATCH\n",
    "    if time() - time_started > print_each_sec:\n",
    "        print('%d: %.3f (%.2f%%) in %.0fs' % (rows_seen, np.mean(costs), acc * 100, time() - time_started))\n",
    "        sys.stdout.flush()\n",
    "        time_started = time()\n",
    "#         acc = check(questions)\n",
    "        acc = 0\n",
    "        costs = []    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
